{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.loaders.data_loader import load_train_data, load_basic_preprocessed_train\n",
    "from src.preprocessors.preprocessors import split_train_valid_test\n",
    "\n",
    "from src.model.multiple_output.convolution import Convolution2DVarious\n",
    "from src.loaders.data_loader import load_test_features, load_basic_preprocessed_predict, load_submission_data\n",
    "from src.loaders.window_generator import WindowGenerator\n",
    "from src.trainers import compile_and_fit_with_pinball_loss\n",
    "\n",
    "df = load_train_data()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.08</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.06</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.78</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.75</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>75.20</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Hour  Minute  DHI  DNI   WS     RH   T  TARGET\n",
       "0    0     0       0    0    0  1.5  69.08 -12     0.0\n",
       "1    0     0      30    0    0  1.5  69.06 -12     0.0\n",
       "2    0     1       0    0    0  1.6  71.78 -12     0.0\n",
       "3    0     1      30    0    0  1.6  71.75 -12     0.0\n",
       "4    0     2       0    0    0  1.6  75.20 -12     0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1fe7b7dd488>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(df[\"DHI\"] + df[\"DNI\"] * -np.cos(df[\"Hour\"] * (2 * np.pi) / 24))[0:48].plot()\n",
    "df[\"TARGET\"][0:48].plot()\n",
    "(df[\"DHI\"] + df[\"DNI\"])[0:48].plot()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio should be 3 length list, example: [0.6, 0.3, 0.1]\n",
      "sum of ration should be 1, example: [0.7, 0.2, 0.1]\n",
      "shape of train, valid, test: (36792, 9), (10511, 9), (5257, 9)\n",
      "ratio should be 3 length list, example: [0.6, 0.3, 0.1]\n",
      "sum of ration should be 1, example: [0.7, 0.2, 0.1]\n",
      "shape of train, valid, test: (36792, 16), (10511, 16), (5257, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>Day_sin</th>\n",
       "      <th>Day_cos</th>\n",
       "      <th>Hour_sin</th>\n",
       "      <th>Hour_cos</th>\n",
       "      <th>GHI</th>\n",
       "      <th>TARGET_ROLLING_MEAN_3_shift_1</th>\n",
       "      <th>TARGET_ROLLING_MEAN_5_shift_2</th>\n",
       "      <th>TARGET_ROLLING_MEAN_11_shift_5</th>\n",
       "      <th>TARGET_ROLLING_MEAN_23_shift_11</th>\n",
       "      <th>TARGET_ROLLING_MEAN_47_shift_23</th>\n",
       "      <th>scaled_TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>3.679200e+04</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>3.679200e+04</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.542351</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>17.439978</td>\n",
       "      <td>0.507046</td>\n",
       "      <td>0.522326</td>\n",
       "      <td>0.500206</td>\n",
       "      <td>0.500027</td>\n",
       "      <td>0.267026</td>\n",
       "      <td>1.754282e-01</td>\n",
       "      <td>0.177078</td>\n",
       "      <td>1.873781e-01</td>\n",
       "      <td>0.239824</td>\n",
       "      <td>0.469365</td>\n",
       "      <td>0.174550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.194232</td>\n",
       "      <td>0.330292</td>\n",
       "      <td>0.117052</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>0.192822</td>\n",
       "      <td>25.449577</td>\n",
       "      <td>0.346981</td>\n",
       "      <td>0.359263</td>\n",
       "      <td>0.353558</td>\n",
       "      <td>0.353558</td>\n",
       "      <td>0.263381</td>\n",
       "      <td>2.493108e-01</td>\n",
       "      <td>0.245816</td>\n",
       "      <td>2.357651e-01</td>\n",
       "      <td>0.221283</td>\n",
       "      <td>0.211497</td>\n",
       "      <td>0.254715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.364896</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159610</td>\n",
       "      <td>0.158025</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.088432</td>\n",
       "      <td>3.356260e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.852145e-16</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>0.297058</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530107</td>\n",
       "      <td>0.536539</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.088432</td>\n",
       "      <td>1.006823e-02</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>7.214131e-02</td>\n",
       "      <td>0.186432</td>\n",
       "      <td>0.447393</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.160985</td>\n",
       "      <td>0.430831</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.711963</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>31.085053</td>\n",
       "      <td>0.840390</td>\n",
       "      <td>0.891488</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>3.152806e-01</td>\n",
       "      <td>0.319378</td>\n",
       "      <td>3.256305e-01</td>\n",
       "      <td>0.378664</td>\n",
       "      <td>0.634329</td>\n",
       "      <td>0.311118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.913939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DHI           DNI            WS            RH             T  \\\n",
       "count  36792.000000  36792.000000  36792.000000  36792.000000  36792.000000   \n",
       "mean       0.119412      0.219731      0.201327      0.542351      0.512100   \n",
       "std        0.194232      0.330292      0.117052      0.234989      0.192822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.116667      0.364896      0.351852   \n",
       "50%        0.000000      0.000000      0.183333      0.553620      0.500000   \n",
       "75%        0.160985      0.430831      0.258333      0.711963      0.648148   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             TARGET       Day_sin       Day_cos      Hour_sin      Hour_cos  \\\n",
       "count  36792.000000  36792.000000  36792.000000  36792.000000  36792.000000   \n",
       "mean      17.439978      0.507046      0.522326      0.500206      0.500027   \n",
       "std       25.449577      0.346981      0.359263      0.353558      0.353558   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.159610      0.158025      0.146447      0.146447   \n",
       "50%        0.000000      0.530107      0.536539      0.500000      0.500000   \n",
       "75%       31.085053      0.840390      0.891488      0.853553      0.853553   \n",
       "max       99.913939      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                GHI  TARGET_ROLLING_MEAN_3_shift_1  \\\n",
       "count  36792.000000                   3.679200e+04   \n",
       "mean       0.267026                   1.754282e-01   \n",
       "std        0.263381                   2.493108e-01   \n",
       "min        0.000000                   0.000000e+00   \n",
       "25%        0.088432                   3.356260e-15   \n",
       "50%        0.088432                   1.006823e-02   \n",
       "75%        0.415900                   3.152806e-01   \n",
       "max        1.000000                   1.000000e+00   \n",
       "\n",
       "       TARGET_ROLLING_MEAN_5_shift_2  TARGET_ROLLING_MEAN_11_shift_5  \\\n",
       "count                   36792.000000                    3.679200e+04   \n",
       "mean                        0.177078                    1.873781e-01   \n",
       "std                         0.245816                    2.357651e-01   \n",
       "min                         0.000000                    0.000000e+00   \n",
       "25%                         0.000000                    1.852145e-16   \n",
       "50%                         0.023250                    7.214131e-02   \n",
       "75%                         0.319378                    3.256305e-01   \n",
       "max                         1.000000                    1.000000e+00   \n",
       "\n",
       "       TARGET_ROLLING_MEAN_23_shift_11  TARGET_ROLLING_MEAN_47_shift_23  \\\n",
       "count                     36792.000000                     36792.000000   \n",
       "mean                          0.239824                         0.469365   \n",
       "std                           0.221283                         0.211497   \n",
       "min                           0.000000                         0.000000   \n",
       "25%                           0.044158                         0.297058   \n",
       "50%                           0.186432                         0.447393   \n",
       "75%                           0.378664                         0.634329   \n",
       "max                           1.000000                         1.000000   \n",
       "\n",
       "       scaled_TARGET  \n",
       "count   36792.000000  \n",
       "mean        0.174550  \n",
       "std         0.254715  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.311118  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df, test_df = split_train_valid_test(df, [0.7, 0.2, 0.1])\n",
    "\n",
    "train_df_target = train_df[\"TARGET\"]\n",
    "valid_df_target = valid_df[\"TARGET\"]\n",
    "test_df_target = test_df[\"TARGET\"]\n",
    "\n",
    "train_df, valid_df, test_df = load_basic_preprocessed_train(\"minmax\")\n",
    "\n",
    "train_df[\"scaled_TARGET\"] = train_df[\"TARGET\"]\n",
    "valid_df[\"scaled_TARGET\"] = valid_df[\"TARGET\"]\n",
    "test_df[\"scaled_TARGET\"] = test_df[\"TARGET\"]\n",
    "\n",
    "train_df[\"TARGET\"] = train_df_target\n",
    "valid_df[\"TARGET\"] = valid_df_target\n",
    "test_df[\"TARGET\"] = test_df_target\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = [\n",
    "    'DHI', \n",
    "    'DNI', \n",
    "#     'WS', \n",
    "#     'RH', \n",
    "#     'T', \n",
    "#     'Hour_sin',\n",
    "#     'Hour_cos', \n",
    "    'GHI', \n",
    "#     'TARGET_ROLLING_MEAN_3_shift_1',\n",
    "#     'TARGET_ROLLING_MEAN_5_shift_2', \n",
    "#     'TARGET_ROLLING_MEAN_11_shift_5',\n",
    "#     'TARGET_ROLLING_MEAN_23_shift_11', \n",
    "#     'TARGET_ROLLING_MEAN_47_shift_23',\n",
    "    'scaled_TARGET',\n",
    "    \"TARGET\",\n",
    "]\n",
    "train_df = train_df[cutter]\n",
    "valid_df = valid_df[cutter]\n",
    "test_df = test_df[cutter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "            Total window size: 336\n",
       "            Input indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
       "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
       "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
       "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
       "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
       "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
       " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
       " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
       " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
       " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
       " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
       " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
       " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
       " 234 235 236 237 238 239]\n",
       "            Label indices: [240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
       " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
       " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
       " 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311\n",
       " 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\n",
       " 330 331 332 333 334 335]\n",
       "            Label column name(s): ['TARGET']\n",
       "        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_days_window_label_columns = WindowGenerator(\n",
    "    train_df,\n",
    "    valid_df,\n",
    "    test_df,\n",
    "    input_width=48 * 5,\n",
    "    label_width=96,\n",
    "    shift=96,\n",
    "    sequence_stride=1,\n",
    "    label_columns=[\"TARGET\"]\n",
    "    \n",
    ")\n",
    "one_days_window_label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio should be 3 length list, example: [0.6, 0.3, 0.1]\n",
      "sum of ration should be 1, example: [0.7, 0.2, 0.1]\n",
      "shape of train, valid, test: (36792, 16), (10511, 16), (5257, 16)\n",
      "0.1\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 27s 15ms/step - loss: 1.6232 - mean_absolute_error: 15.3887 - mean_squared_error: 756.7950 - _pinball_loss: 1.6232 - val_loss: 1.8743 - val_mean_absolute_error: 16.6931 - val_mean_squared_error: 784.1779 - val__pinball_loss: 1.8743\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.4617 - mean_absolute_error: 12.8708 - mean_squared_error: 538.5213 - _pinball_loss: 1.4617 - val_loss: 1.8323 - val_mean_absolute_error: 15.8554 - val_mean_squared_error: 712.3745 - val__pinball_loss: 1.8323\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.4287 - mean_absolute_error: 12.3571 - mean_squared_error: 498.7604 - _pinball_loss: 1.4287 - val_loss: 1.8166 - val_mean_absolute_error: 15.5750 - val_mean_squared_error: 691.3523 - val__pinball_loss: 1.8166\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.4154 - mean_absolute_error: 12.1649 - mean_squared_error: 485.1922 - _pinball_loss: 1.4154 - val_loss: 1.8067 - val_mean_absolute_error: 15.4271 - val_mean_squared_error: 682.6071 - val__pinball_loss: 1.8067\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.4059 - mean_absolute_error: 12.0264 - mean_squared_error: 475.6208 - _pinball_loss: 1.4059 - val_loss: 1.7979 - val_mean_absolute_error: 15.3205 - val_mean_squared_error: 674.9983 - val__pinball_loss: 1.7979\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3980 - mean_absolute_error: 11.9157 - mean_squared_error: 468.1656 - _pinball_loss: 1.3980 - val_loss: 1.7950 - val_mean_absolute_error: 15.2537 - val_mean_squared_error: 670.0940 - val__pinball_loss: 1.7950\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3932 - mean_absolute_error: 11.8387 - mean_squared_error: 463.4454 - _pinball_loss: 1.3932 - val_loss: 1.7892 - val_mean_absolute_error: 15.2615 - val_mean_squared_error: 671.7218 - val__pinball_loss: 1.7892\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3891 - mean_absolute_error: 11.7914 - mean_squared_error: 461.0357 - _pinball_loss: 1.3891 - val_loss: 1.7856 - val_mean_absolute_error: 15.2306 - val_mean_squared_error: 669.9210 - val__pinball_loss: 1.7856\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3853 - mean_absolute_error: 11.7358 - mean_squared_error: 457.7079 - _pinball_loss: 1.3853 - val_loss: 1.7884 - val_mean_absolute_error: 15.3226 - val_mean_squared_error: 675.3127 - val__pinball_loss: 1.7884\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3820 - mean_absolute_error: 11.7053 - mean_squared_error: 456.1219 - _pinball_loss: 1.3820 - val_loss: 1.7818 - val_mean_absolute_error: 15.0732 - val_mean_squared_error: 658.5239 - val__pinball_loss: 1.7818\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3790 - mean_absolute_error: 11.6610 - mean_squared_error: 453.5793 - _pinball_loss: 1.3790 - val_loss: 1.7763 - val_mean_absolute_error: 14.8536 - val_mean_squared_error: 644.9444 - val__pinball_loss: 1.7763\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3754 - mean_absolute_error: 11.6065 - mean_squared_error: 450.5831 - _pinball_loss: 1.3754 - val_loss: 1.7744 - val_mean_absolute_error: 15.0848 - val_mean_squared_error: 661.1569 - val__pinball_loss: 1.7744\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3730 - mean_absolute_error: 11.6006 - mean_squared_error: 450.5926 - _pinball_loss: 1.3730 - val_loss: 1.7722 - val_mean_absolute_error: 14.9916 - val_mean_squared_error: 656.7253 - val__pinball_loss: 1.7722\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3709 - mean_absolute_error: 11.5850 - mean_squared_error: 449.8303 - _pinball_loss: 1.3709 - val_loss: 1.7712 - val_mean_absolute_error: 14.9651 - val_mean_squared_error: 652.8572 - val__pinball_loss: 1.7712\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3695 - mean_absolute_error: 11.5412 - mean_squared_error: 447.2410 - _pinball_loss: 1.3695 - val_loss: 1.7694 - val_mean_absolute_error: 14.8942 - val_mean_squared_error: 650.9947 - val__pinball_loss: 1.7694\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3670 - mean_absolute_error: 11.5122 - mean_squared_error: 445.9555 - _pinball_loss: 1.3670 - val_loss: 1.7682 - val_mean_absolute_error: 14.8614 - val_mean_squared_error: 647.3201 - val__pinball_loss: 1.7682\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3655 - mean_absolute_error: 11.5070 - mean_squared_error: 445.8587 - _pinball_loss: 1.3655 - val_loss: 1.7675 - val_mean_absolute_error: 14.9128 - val_mean_squared_error: 653.0804 - val__pinball_loss: 1.7675\n",
      "Epoch 18/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3648 - mean_absolute_error: 11.5022 - mean_squared_error: 445.7689 - _pinball_loss: 1.3648 - val_loss: 1.7659 - val_mean_absolute_error: 14.9046 - val_mean_squared_error: 652.5588 - val__pinball_loss: 1.7659\n",
      "Epoch 19/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3627 - mean_absolute_error: 11.4714 - mean_squared_error: 444.0617 - _pinball_loss: 1.3627 - val_loss: 1.7659 - val_mean_absolute_error: 14.9111 - val_mean_squared_error: 652.7022 - val__pinball_loss: 1.7659\n",
      "Epoch 20/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3624 - mean_absolute_error: 11.4624 - mean_squared_error: 443.7061 - _pinball_loss: 1.3624 - val_loss: 1.7663 - val_mean_absolute_error: 14.8833 - val_mean_squared_error: 650.0184 - val__pinball_loss: 1.7663\n",
      "Epoch 21/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3607 - mean_absolute_error: 11.4446 - mean_squared_error: 442.6219 - _pinball_loss: 1.3607 - val_loss: 1.7634 - val_mean_absolute_error: 14.7962 - val_mean_squared_error: 645.8584 - val__pinball_loss: 1.7634\n",
      "Epoch 22/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3601 - mean_absolute_error: 11.4472 - mean_squared_error: 443.0791 - _pinball_loss: 1.3601 - val_loss: 1.7638 - val_mean_absolute_error: 14.8897 - val_mean_squared_error: 651.8724 - val__pinball_loss: 1.7638\n",
      "Epoch 23/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3585 - mean_absolute_error: 11.4271 - mean_squared_error: 442.1014 - _pinball_loss: 1.3585 - val_loss: 1.7623 - val_mean_absolute_error: 14.7709 - val_mean_squared_error: 641.7634 - val__pinball_loss: 1.7623\n",
      "Epoch 24/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3582 - mean_absolute_error: 11.4132 - mean_squared_error: 441.2181 - _pinball_loss: 1.3582 - val_loss: 1.7620 - val_mean_absolute_error: 14.8711 - val_mean_squared_error: 649.0427 - val__pinball_loss: 1.7620\n",
      "Epoch 25/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3562 - mean_absolute_error: 11.3985 - mean_squared_error: 440.6436 - _pinball_loss: 1.3562 - val_loss: 1.7610 - val_mean_absolute_error: 14.7829 - val_mean_squared_error: 645.0656 - val__pinball_loss: 1.7610\n",
      "Epoch 26/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3560 - mean_absolute_error: 11.3901 - mean_squared_error: 440.2373 - _pinball_loss: 1.3560 - val_loss: 1.7620 - val_mean_absolute_error: 14.8631 - val_mean_squared_error: 648.6773 - val__pinball_loss: 1.7620\n",
      "Epoch 27/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3545 - mean_absolute_error: 11.3661 - mean_squared_error: 438.7190 - _pinball_loss: 1.3545 - val_loss: 1.7612 - val_mean_absolute_error: 14.8539 - val_mean_squared_error: 650.7760 - val__pinball_loss: 1.7612\n",
      "Epoch 28/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 1.3545 - mean_absolute_error: 11.3712 - mean_squared_error: 439.0997 - _pinball_loss: 1.3545 - val_loss: 1.7612 - val_mean_absolute_error: 14.6497 - val_mean_squared_error: 634.8804 - val__pinball_loss: 1.7612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.9931 - mean_absolute_error: 13.6716 - mean_squared_error: 622.8927 - _pinball_loss: 2.9931 - val_loss: 3.1828 - val_mean_absolute_error: 13.1840 - val_mean_squared_error: 508.1332 - val__pinball_loss: 3.1828\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.4908 - mean_absolute_error: 10.1198 - mean_squared_error: 347.4274 - _pinball_loss: 2.4908 - val_loss: 3.0886 - val_mean_absolute_error: 12.4211 - val_mean_squared_error: 453.6729 - val__pinball_loss: 3.0886\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.4124 - mean_absolute_error: 9.5818 - mean_squared_error: 311.6272 - _pinball_loss: 2.4124 - val_loss: 3.0385 - val_mean_absolute_error: 12.0010 - val_mean_squared_error: 425.3909 - val__pinball_loss: 3.0385\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.3656 - mean_absolute_error: 9.2774 - mean_squared_error: 292.5747 - _pinball_loss: 2.3656 - val_loss: 3.0086 - val_mean_absolute_error: 11.6810 - val_mean_squared_error: 407.6421 - val__pinball_loss: 3.0086\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.3351 - mean_absolute_error: 9.0623 - mean_squared_error: 280.5969 - _pinball_loss: 2.3351 - val_loss: 2.9952 - val_mean_absolute_error: 11.7069 - val_mean_squared_error: 406.9064 - val__pinball_loss: 2.9952\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.3138 - mean_absolute_error: 8.9368 - mean_squared_error: 274.1300 - _pinball_loss: 2.3138 - val_loss: 2.9767 - val_mean_absolute_error: 11.3710 - val_mean_squared_error: 389.7117 - val__pinball_loss: 2.9767\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2972 - mean_absolute_error: 8.8331 - mean_squared_error: 269.5263 - _pinball_loss: 2.2972 - val_loss: 2.9653 - val_mean_absolute_error: 11.3614 - val_mean_squared_error: 391.1167 - val__pinball_loss: 2.9653\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2867 - mean_absolute_error: 8.7651 - mean_squared_error: 266.4730 - _pinball_loss: 2.2867 - val_loss: 2.9553 - val_mean_absolute_error: 11.1022 - val_mean_squared_error: 377.9984 - val__pinball_loss: 2.9553\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2809 - mean_absolute_error: 8.7286 - mean_squared_error: 264.8223 - _pinball_loss: 2.2809 - val_loss: 2.9538 - val_mean_absolute_error: 11.2376 - val_mean_squared_error: 381.9229 - val__pinball_loss: 2.9538\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2720 - mean_absolute_error: 8.6942 - mean_squared_error: 263.7590 - _pinball_loss: 2.2720 - val_loss: 2.9420 - val_mean_absolute_error: 11.0999 - val_mean_squared_error: 378.9117 - val__pinball_loss: 2.9420\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2653 - mean_absolute_error: 8.6460 - mean_squared_error: 261.7308 - _pinball_loss: 2.2653 - val_loss: 2.9324 - val_mean_absolute_error: 11.1877 - val_mean_squared_error: 382.9056 - val__pinball_loss: 2.9324\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2594 - mean_absolute_error: 8.6084 - mean_squared_error: 259.8821 - _pinball_loss: 2.2594 - val_loss: 2.9310 - val_mean_absolute_error: 11.3435 - val_mean_squared_error: 392.4032 - val__pinball_loss: 2.9310\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2556 - mean_absolute_error: 8.5888 - mean_squared_error: 259.2930 - _pinball_loss: 2.2556 - val_loss: 2.9223 - val_mean_absolute_error: 11.2191 - val_mean_squared_error: 385.4905 - val__pinball_loss: 2.9223\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2492 - mean_absolute_error: 8.5631 - mean_squared_error: 258.3063 - _pinball_loss: 2.2492 - val_loss: 2.9227 - val_mean_absolute_error: 11.3611 - val_mean_squared_error: 392.8332 - val__pinball_loss: 2.9227\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2453 - mean_absolute_error: 8.5475 - mean_squared_error: 257.5924 - _pinball_loss: 2.2453 - val_loss: 2.9198 - val_mean_absolute_error: 11.1499 - val_mean_squared_error: 381.1502 - val__pinball_loss: 2.9198\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2410 - mean_absolute_error: 8.5195 - mean_squared_error: 256.3805 - _pinball_loss: 2.2410 - val_loss: 2.9092 - val_mean_absolute_error: 10.9447 - val_mean_squared_error: 371.7122 - val__pinball_loss: 2.9092\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2374 - mean_absolute_error: 8.4902 - mean_squared_error: 254.9814 - _pinball_loss: 2.2374 - val_loss: 2.9026 - val_mean_absolute_error: 11.1814 - val_mean_squared_error: 384.9086 - val__pinball_loss: 2.9026\n",
      "Epoch 18/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2347 - mean_absolute_error: 8.4808 - mean_squared_error: 254.5453 - _pinball_loss: 2.2347 - val_loss: 2.9048 - val_mean_absolute_error: 11.2064 - val_mean_squared_error: 384.9584 - val__pinball_loss: 2.9048\n",
      "Epoch 19/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2338 - mean_absolute_error: 8.4930 - mean_squared_error: 255.0482 - _pinball_loss: 2.2338 - val_loss: 2.9044 - val_mean_absolute_error: 11.2773 - val_mean_squared_error: 386.4568 - val__pinball_loss: 2.9044\n",
      "Epoch 20/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.2307 - mean_absolute_error: 8.4705 - mean_squared_error: 254.1570 - _pinball_loss: 2.2307 - val_loss: 2.9027 - val_mean_absolute_error: 11.3482 - val_mean_squared_error: 391.5385 - val__pinball_loss: 2.9027\n",
      "0.3\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 3.8266 - mean_absolute_error: 11.2176 - mean_squared_error: 441.2722 - _pinball_loss: 3.8266 - val_loss: 3.6067 - val_mean_absolute_error: 8.8055 - val_mean_squared_error: 233.9429 - val__pinball_loss: 3.6067\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.8105 - mean_absolute_error: 7.0142 - mean_squared_error: 166.0565 - _pinball_loss: 2.8105 - val_loss: 3.5427 - val_mean_absolute_error: 8.6633 - val_mean_squared_error: 234.9467 - val__pinball_loss: 3.5427\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.7494 - mean_absolute_error: 6.8243 - mean_squared_error: 162.2997 - _pinball_loss: 2.7494 - val_loss: 3.5090 - val_mean_absolute_error: 8.5188 - val_mean_squared_error: 230.7236 - val__pinball_loss: 3.5090\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.7144 - mean_absolute_error: 6.7109 - mean_squared_error: 159.2122 - _pinball_loss: 2.7144 - val_loss: 3.4785 - val_mean_absolute_error: 8.5574 - val_mean_squared_error: 233.4672 - val__pinball_loss: 3.4785\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6920 - mean_absolute_error: 6.6436 - mean_squared_error: 157.1022 - _pinball_loss: 2.6920 - val_loss: 3.4675 - val_mean_absolute_error: 8.5403 - val_mean_squared_error: 231.2985 - val__pinball_loss: 3.4675\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6747 - mean_absolute_error: 6.5893 - mean_squared_error: 155.5377 - _pinball_loss: 2.6747 - val_loss: 3.4634 - val_mean_absolute_error: 8.6468 - val_mean_squared_error: 237.1222 - val__pinball_loss: 3.4634\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6608 - mean_absolute_error: 6.5551 - mean_squared_error: 154.4714 - _pinball_loss: 2.6608 - val_loss: 3.4697 - val_mean_absolute_error: 8.4003 - val_mean_squared_error: 228.5989 - val__pinball_loss: 3.4697\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6518 - mean_absolute_error: 6.5099 - mean_squared_error: 153.3508 - _pinball_loss: 2.6518 - val_loss: 3.4526 - val_mean_absolute_error: 8.5358 - val_mean_squared_error: 232.1217 - val__pinball_loss: 3.4526\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6378 - mean_absolute_error: 6.4874 - mean_squared_error: 152.6186 - _pinball_loss: 2.6378 - val_loss: 3.4402 - val_mean_absolute_error: 8.4598 - val_mean_squared_error: 230.0593 - val__pinball_loss: 3.4402\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6324 - mean_absolute_error: 6.4631 - mean_squared_error: 151.8536 - _pinball_loss: 2.6324 - val_loss: 3.4493 - val_mean_absolute_error: 8.4160 - val_mean_squared_error: 228.7889 - val__pinball_loss: 3.4493\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6235 - mean_absolute_error: 6.4401 - mean_squared_error: 151.1213 - _pinball_loss: 2.6235 - val_loss: 3.4433 - val_mean_absolute_error: 8.4844 - val_mean_squared_error: 231.3889 - val__pinball_loss: 3.4433\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6142 - mean_absolute_error: 6.4042 - mean_squared_error: 149.9249 - _pinball_loss: 2.6142 - val_loss: 3.4311 - val_mean_absolute_error: 8.4018 - val_mean_squared_error: 228.5888 - val__pinball_loss: 3.4311\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.6027 - mean_absolute_error: 6.3768 - mean_squared_error: 149.0786 - _pinball_loss: 2.6027 - val_loss: 3.4557 - val_mean_absolute_error: 8.4628 - val_mean_squared_error: 228.6499 - val__pinball_loss: 3.4557\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5932 - mean_absolute_error: 6.3484 - mean_squared_error: 147.9340 - _pinball_loss: 2.5932 - val_loss: 3.4364 - val_mean_absolute_error: 8.2589 - val_mean_squared_error: 224.9195 - val__pinball_loss: 3.4364\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5848 - mean_absolute_error: 6.3243 - mean_squared_error: 147.1297 - _pinball_loss: 2.5848 - val_loss: 3.4275 - val_mean_absolute_error: 8.3715 - val_mean_squared_error: 228.1126 - val__pinball_loss: 3.4275\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5755 - mean_absolute_error: 6.3022 - mean_squared_error: 146.3164 - _pinball_loss: 2.5755 - val_loss: 3.4252 - val_mean_absolute_error: 8.3304 - val_mean_squared_error: 226.1505 - val__pinball_loss: 3.4252\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5659 - mean_absolute_error: 6.2822 - mean_squared_error: 145.5376 - _pinball_loss: 2.5659 - val_loss: 3.4175 - val_mean_absolute_error: 8.3134 - val_mean_squared_error: 225.7749 - val__pinball_loss: 3.4175\n",
      "Epoch 18/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5562 - mean_absolute_error: 6.2537 - mean_squared_error: 144.4294 - _pinball_loss: 2.5562 - val_loss: 3.4154 - val_mean_absolute_error: 8.3021 - val_mean_squared_error: 224.4724 - val__pinball_loss: 3.4154\n",
      "Epoch 19/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5427 - mean_absolute_error: 6.2192 - mean_squared_error: 143.1779 - _pinball_loss: 2.5427 - val_loss: 3.4205 - val_mean_absolute_error: 8.3483 - val_mean_squared_error: 227.5364 - val__pinball_loss: 3.4205\n",
      "Epoch 20/1000\n",
      "1137/1137 [==============================] - 9s 8ms/step - loss: 2.5350 - mean_absolute_error: 6.2032 - mean_squared_error: 142.3833 - _pinball_loss: 2.5350 - val_loss: 3.4369 - val_mean_absolute_error: 8.3504 - val_mean_squared_error: 224.5075 - val__pinball_loss: 3.4369\n",
      "Epoch 21/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.5239 - mean_absolute_error: 6.1730 - mean_squared_error: 141.1728 - _pinball_loss: 2.5239 - val_loss: 3.4206 - val_mean_absolute_error: 8.3746 - val_mean_squared_error: 228.1302 - val__pinball_loss: 3.4206\n",
      "0.4\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 4.2944 - mean_absolute_error: 9.6200 - mean_squared_error: 329.2467 - _pinball_loss: 4.2944 - val_loss: 3.7782 - val_mean_absolute_error: 7.8009 - val_mean_squared_error: 197.9401 - val__pinball_loss: 3.7782\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.9708 - mean_absolute_error: 6.0697 - mean_squared_error: 136.9017 - _pinball_loss: 2.9708 - val_loss: 3.6882 - val_mean_absolute_error: 7.4897 - val_mean_squared_error: 193.6720 - val__pinball_loss: 3.6882\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.9038 - mean_absolute_error: 5.9098 - mean_squared_error: 133.6939 - _pinball_loss: 2.9038 - val_loss: 3.6530 - val_mean_absolute_error: 7.4472 - val_mean_squared_error: 190.4050 - val__pinball_loss: 3.6530\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.8628 - mean_absolute_error: 5.8135 - mean_squared_error: 131.3250 - _pinball_loss: 2.8628 - val_loss: 3.6348 - val_mean_absolute_error: 7.3510 - val_mean_squared_error: 189.2200 - val__pinball_loss: 3.6348\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.8361 - mean_absolute_error: 5.7499 - mean_squared_error: 129.8204 - _pinball_loss: 2.8361 - val_loss: 3.6145 - val_mean_absolute_error: 7.2726 - val_mean_squared_error: 188.7664 - val__pinball_loss: 3.6145\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 10s 8ms/step - loss: 2.8132 - mean_absolute_error: 5.6993 - mean_squared_error: 128.5162 - _pinball_loss: 2.8132 - val_loss: 3.6062 - val_mean_absolute_error: 7.3329 - val_mean_squared_error: 188.9692 - val__pinball_loss: 3.6062\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7936 - mean_absolute_error: 5.6534 - mean_squared_error: 127.5686 - _pinball_loss: 2.7936 - val_loss: 3.5895 - val_mean_absolute_error: 7.2709 - val_mean_squared_error: 188.1177 - val__pinball_loss: 3.5895\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7811 - mean_absolute_error: 5.6263 - mean_squared_error: 126.9643 - _pinball_loss: 2.7811 - val_loss: 3.5913 - val_mean_absolute_error: 7.2183 - val_mean_squared_error: 187.8553 - val__pinball_loss: 3.5913\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7678 - mean_absolute_error: 5.5964 - mean_squared_error: 126.1987 - _pinball_loss: 2.7678 - val_loss: 3.6003 - val_mean_absolute_error: 7.4015 - val_mean_squared_error: 188.8309 - val__pinball_loss: 3.6003\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7578 - mean_absolute_error: 5.5803 - mean_squared_error: 125.7223 - _pinball_loss: 2.7578 - val_loss: 3.5808 - val_mean_absolute_error: 7.2542 - val_mean_squared_error: 187.4960 - val__pinball_loss: 3.5808\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7449 - mean_absolute_error: 5.5477 - mean_squared_error: 124.9444 - _pinball_loss: 2.7449 - val_loss: 3.5800 - val_mean_absolute_error: 7.2578 - val_mean_squared_error: 188.6678 - val__pinball_loss: 3.5800\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7333 - mean_absolute_error: 5.5220 - mean_squared_error: 124.3183 - _pinball_loss: 2.7333 - val_loss: 3.5759 - val_mean_absolute_error: 7.2395 - val_mean_squared_error: 188.6782 - val__pinball_loss: 3.5759\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7242 - mean_absolute_error: 5.5047 - mean_squared_error: 123.7854 - _pinball_loss: 2.7242 - val_loss: 3.5708 - val_mean_absolute_error: 7.2639 - val_mean_squared_error: 189.0061 - val__pinball_loss: 3.5708\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7135 - mean_absolute_error: 5.4810 - mean_squared_error: 123.2240 - _pinball_loss: 2.7135 - val_loss: 3.5755 - val_mean_absolute_error: 7.1663 - val_mean_squared_error: 189.7989 - val__pinball_loss: 3.5755\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7015 - mean_absolute_error: 5.4500 - mean_squared_error: 122.5173 - _pinball_loss: 2.7015 - val_loss: 3.5771 - val_mean_absolute_error: 7.2209 - val_mean_squared_error: 189.8666 - val__pinball_loss: 3.5771\n",
      "Epoch 16/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.6897 - mean_absolute_error: 5.4287 - mean_squared_error: 121.7725 - _pinball_loss: 2.6897 - val_loss: 3.5886 - val_mean_absolute_error: 7.3294 - val_mean_squared_error: 190.6151 - val__pinball_loss: 3.5886\n",
      "0.5\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 4.6211 - mean_absolute_error: 9.2423 - mean_squared_error: 305.1378 - _pinball_loss: 4.6211 - val_loss: 3.6813 - val_mean_absolute_error: 7.3626 - val_mean_squared_error: 204.1061 - val__pinball_loss: 3.6813\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.8850 - mean_absolute_error: 5.7700 - mean_squared_error: 141.2110 - _pinball_loss: 2.8850 - val_loss: 3.5710 - val_mean_absolute_error: 7.1420 - val_mean_squared_error: 201.4057 - val__pinball_loss: 3.5710\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.8025 - mean_absolute_error: 5.6050 - mean_squared_error: 138.1289 - _pinball_loss: 2.8025 - val_loss: 3.5250 - val_mean_absolute_error: 7.0501 - val_mean_squared_error: 201.4678 - val__pinball_loss: 3.5250\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7502 - mean_absolute_error: 5.5005 - mean_squared_error: 136.2591 - _pinball_loss: 2.7502 - val_loss: 3.4961 - val_mean_absolute_error: 6.9921 - val_mean_squared_error: 197.8115 - val__pinball_loss: 3.4961\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.7198 - mean_absolute_error: 5.4395 - mean_squared_error: 135.0609 - _pinball_loss: 2.7198 - val_loss: 3.4838 - val_mean_absolute_error: 6.9675 - val_mean_squared_error: 197.5977 - val__pinball_loss: 3.4838\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 2.6953 - mean_absolute_error: 5.3906 - mean_squared_error: 133.9729 - _pinball_loss: 2.6953 - val_loss: 3.4609 - val_mean_absolute_error: 6.9218 - val_mean_squared_error: 194.7917 - val__pinball_loss: 3.4609\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 2.6768 - mean_absolute_error: 5.3537 - mean_squared_error: 133.1900 - _pinball_loss: 2.6768 - val_loss: 3.4637 - val_mean_absolute_error: 6.9275 - val_mean_squared_error: 194.9146 - val__pinball_loss: 3.4637\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 2.6624 - mean_absolute_error: 5.3247 - mean_squared_error: 132.4288 - _pinball_loss: 2.6624 - val_loss: 3.4638 - val_mean_absolute_error: 6.9276 - val_mean_squared_error: 198.9880 - val__pinball_loss: 3.4638\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.6538 - mean_absolute_error: 5.3077 - mean_squared_error: 132.3037 - _pinball_loss: 2.6538 - val_loss: 3.4571 - val_mean_absolute_error: 6.9142 - val_mean_squared_error: 193.6359 - val__pinball_loss: 3.4571\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 2.6415 - mean_absolute_error: 5.2830 - mean_squared_error: 131.4886 - _pinball_loss: 2.6415 - val_loss: 3.4479 - val_mean_absolute_error: 6.8959 - val_mean_squared_error: 198.6951 - val__pinball_loss: 3.4479\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 2.6322 - mean_absolute_error: 5.2644 - mean_squared_error: 131.0206 - _pinball_loss: 2.6322 - val_loss: 3.4367 - val_mean_absolute_error: 6.8734 - val_mean_squared_error: 199.2550 - val__pinball_loss: 3.4367\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 2.6186 - mean_absolute_error: 5.2373 - mean_squared_error: 130.3934 - _pinball_loss: 2.6186 - val_loss: 3.4400 - val_mean_absolute_error: 6.8800 - val_mean_squared_error: 196.8830 - val__pinball_loss: 3.4400\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 12s 10ms/step - loss: 2.6163 - mean_absolute_error: 5.2326 - mean_squared_error: 130.0451 - _pinball_loss: 2.6163 - val_loss: 3.4398 - val_mean_absolute_error: 6.8797 - val_mean_squared_error: 199.8418 - val__pinball_loss: 3.4398\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.6007 - mean_absolute_error: 5.2014 - mean_squared_error: 129.5137 - _pinball_loss: 2.6007 - val_loss: 3.4830 - val_mean_absolute_error: 6.9660 - val_mean_squared_error: 190.1630 - val__pinball_loss: 3.4830\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000200F09D3708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.6\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 4.7490 - mean_absolute_error: 9.5476 - mean_squared_error: 295.3102 - _pinball_loss: 4.7490 - val_loss: 3.4030 - val_mean_absolute_error: 7.6459 - val_mean_squared_error: 234.5149 - val__pinball_loss: 3.4030\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.6774 - mean_absolute_error: 5.9964 - mean_squared_error: 160.9292 - _pinball_loss: 2.6774 - val_loss: 3.2597 - val_mean_absolute_error: 7.2821 - val_mean_squared_error: 226.0755 - val__pinball_loss: 3.2597\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.5705 - mean_absolute_error: 5.7910 - mean_squared_error: 158.5084 - _pinball_loss: 2.5705 - val_loss: 3.2287 - val_mean_absolute_error: 7.2371 - val_mean_squared_error: 226.5222 - val__pinball_loss: 3.2287\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.5187 - mean_absolute_error: 5.6969 - mean_squared_error: 157.5711 - _pinball_loss: 2.5187 - val_loss: 3.1729 - val_mean_absolute_error: 7.0992 - val_mean_squared_error: 223.3490 - val__pinball_loss: 3.1729\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.4798 - mean_absolute_error: 5.6225 - mean_squared_error: 156.4172 - _pinball_loss: 2.4798 - val_loss: 3.1613 - val_mean_absolute_error: 7.1033 - val_mean_squared_error: 223.9899 - val__pinball_loss: 3.1613\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.4502 - mean_absolute_error: 5.5658 - mean_squared_error: 155.4396 - _pinball_loss: 2.4502 - val_loss: 3.1316 - val_mean_absolute_error: 7.0875 - val_mean_squared_error: 225.5930 - val__pinball_loss: 3.1316\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.4330 - mean_absolute_error: 5.5366 - mean_squared_error: 155.1676 - _pinball_loss: 2.4330 - val_loss: 3.1178 - val_mean_absolute_error: 7.0516 - val_mean_squared_error: 224.2204 - val__pinball_loss: 3.1178\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.4190 - mean_absolute_error: 5.5092 - mean_squared_error: 154.6761 - _pinball_loss: 2.4190 - val_loss: 3.1251 - val_mean_absolute_error: 7.1191 - val_mean_squared_error: 231.8653 - val__pinball_loss: 3.1251\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.4084 - mean_absolute_error: 5.4882 - mean_squared_error: 154.1166 - _pinball_loss: 2.4084 - val_loss: 3.1118 - val_mean_absolute_error: 7.0287 - val_mean_squared_error: 221.7213 - val__pinball_loss: 3.1118\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.3968 - mean_absolute_error: 5.4650 - mean_squared_error: 153.4944 - _pinball_loss: 2.3968 - val_loss: 3.1072 - val_mean_absolute_error: 7.0156 - val_mean_squared_error: 222.4731 - val__pinball_loss: 3.1072\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.3896 - mean_absolute_error: 5.4507 - mean_squared_error: 153.0512 - _pinball_loss: 2.3896 - val_loss: 3.1014 - val_mean_absolute_error: 7.0161 - val_mean_squared_error: 221.2731 - val__pinball_loss: 3.1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.3826 - mean_absolute_error: 5.4386 - mean_squared_error: 153.0287 - _pinball_loss: 2.3826 - val_loss: 3.0900 - val_mean_absolute_error: 6.9547 - val_mean_squared_error: 218.8538 - val__pinball_loss: 3.0900\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 2.3703 - mean_absolute_error: 5.4140 - mean_squared_error: 152.4644 - _pinball_loss: 2.3703 - val_loss: 3.0965 - val_mean_absolute_error: 7.0490 - val_mean_squared_error: 226.0508 - val__pinball_loss: 3.0965\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 2.3682 - mean_absolute_error: 5.4119 - mean_squared_error: 152.5097 - _pinball_loss: 2.3682 - val_loss: 3.0759 - val_mean_absolute_error: 6.9573 - val_mean_squared_error: 221.7318 - val__pinball_loss: 3.0759\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 12s 11ms/step - loss: 2.3584 - mean_absolute_error: 5.3912 - mean_squared_error: 151.8020 - _pinball_loss: 2.3584 - val_loss: 3.1020 - val_mean_absolute_error: 7.0205 - val_mean_squared_error: 221.5781 - val__pinball_loss: 3.1020\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.3481 - mean_absolute_error: 5.3724 - mean_squared_error: 151.7156 - _pinball_loss: 2.3481 - val_loss: 3.0826 - val_mean_absolute_error: 6.9617 - val_mean_squared_error: 221.1379 - val__pinball_loss: 3.0826\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.3434 - mean_absolute_error: 5.3601 - mean_squared_error: 151.0378 - _pinball_loss: 2.3434 - val_loss: 3.0776 - val_mean_absolute_error: 6.9454 - val_mean_squared_error: 221.3930 - val__pinball_loss: 3.0776\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002011A084318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.7\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 4.5180 - mean_absolute_error: 10.3991 - mean_squared_error: 311.2695 - _pinball_loss: 4.5180 - val_loss: 2.9147 - val_mean_absolute_error: 8.3752 - val_mean_squared_error: 279.9073 - val__pinball_loss: 2.9147\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.2931 - mean_absolute_error: 6.4910 - mean_squared_error: 187.2728 - _pinball_loss: 2.2931 - val_loss: 2.7441 - val_mean_absolute_error: 7.8769 - val_mean_squared_error: 270.3752 - val__pinball_loss: 2.7441\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.1723 - mean_absolute_error: 6.2226 - mean_squared_error: 184.3992 - _pinball_loss: 2.1723 - val_loss: 2.6867 - val_mean_absolute_error: 7.8353 - val_mean_squared_error: 272.6558 - val__pinball_loss: 2.6867\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.1102 - mean_absolute_error: 6.0910 - mean_squared_error: 183.1898 - _pinball_loss: 2.1102 - val_loss: 2.6462 - val_mean_absolute_error: 7.6003 - val_mean_squared_error: 262.9258 - val__pinball_loss: 2.6462\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.0719 - mean_absolute_error: 6.0055 - mean_squared_error: 181.8492 - _pinball_loss: 2.0719 - val_loss: 2.6278 - val_mean_absolute_error: 7.6448 - val_mean_squared_error: 264.1360 - val__pinball_loss: 2.6278\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.0462 - mean_absolute_error: 5.9512 - mean_squared_error: 180.9582 - _pinball_loss: 2.0462 - val_loss: 2.6212 - val_mean_absolute_error: 7.4461 - val_mean_squared_error: 257.4980 - val__pinball_loss: 2.6212\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 2.0273 - mean_absolute_error: 5.9120 - mean_squared_error: 180.7472 - _pinball_loss: 2.0273 - val_loss: 2.6030 - val_mean_absolute_error: 7.5652 - val_mean_squared_error: 263.6738 - val__pinball_loss: 2.6030\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 2.0075 - mean_absolute_error: 5.8711 - mean_squared_error: 180.1212 - _pinball_loss: 2.0075 - val_loss: 2.6002 - val_mean_absolute_error: 7.6739 - val_mean_squared_error: 270.7211 - val__pinball_loss: 2.6002\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 10s 9ms/step - loss: 1.9923 - mean_absolute_error: 5.8451 - mean_squared_error: 180.1174 - _pinball_loss: 1.9923 - val_loss: 2.5897 - val_mean_absolute_error: 7.6255 - val_mean_squared_error: 270.6423 - val__pinball_loss: 2.5897\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9789 - mean_absolute_error: 5.8131 - mean_squared_error: 179.3285 - _pinball_loss: 1.9789 - val_loss: 2.5832 - val_mean_absolute_error: 7.5062 - val_mean_squared_error: 262.2200 - val__pinball_loss: 2.5832\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9676 - mean_absolute_error: 5.7943 - mean_squared_error: 179.1226 - _pinball_loss: 1.9676 - val_loss: 2.5634 - val_mean_absolute_error: 7.5073 - val_mean_squared_error: 264.8474 - val__pinball_loss: 2.5634\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9554 - mean_absolute_error: 5.7675 - mean_squared_error: 178.7400 - _pinball_loss: 1.9554 - val_loss: 2.5597 - val_mean_absolute_error: 7.3039 - val_mean_squared_error: 252.8971 - val__pinball_loss: 2.5597\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9452 - mean_absolute_error: 5.7404 - mean_squared_error: 177.9791 - _pinball_loss: 1.9452 - val_loss: 2.5386 - val_mean_absolute_error: 7.3486 - val_mean_squared_error: 258.4944 - val__pinball_loss: 2.5386\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9328 - mean_absolute_error: 5.7173 - mean_squared_error: 177.7054 - _pinball_loss: 1.9328 - val_loss: 2.5328 - val_mean_absolute_error: 7.3962 - val_mean_squared_error: 262.4586 - val__pinball_loss: 2.5328\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9240 - mean_absolute_error: 5.6973 - mean_squared_error: 177.2419 - _pinball_loss: 1.9240 - val_loss: 2.5481 - val_mean_absolute_error: 7.3541 - val_mean_squared_error: 256.8559 - val__pinball_loss: 2.5481\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9184 - mean_absolute_error: 5.6874 - mean_squared_error: 177.0732 - _pinball_loss: 1.9184 - val_loss: 2.5457 - val_mean_absolute_error: 7.3290 - val_mean_squared_error: 257.1815 - val__pinball_loss: 2.5457\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.9081 - mean_absolute_error: 5.6620 - mean_squared_error: 176.2332 - _pinball_loss: 1.9081 - val_loss: 2.5565 - val_mean_absolute_error: 7.5878 - val_mean_squared_error: 272.6357 - val__pinball_loss: 2.5565\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000200DD17E828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.8\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 11s 9ms/step - loss: 4.2972 - mean_absolute_error: 13.0073 - mean_squared_error: 415.9955 - _pinball_loss: 4.2972 - val_loss: 2.1780 - val_mean_absolute_error: 9.2142 - val_mean_squared_error: 326.9999 - val__pinball_loss: 2.1780\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.7377 - mean_absolute_error: 7.3063 - mean_squared_error: 223.2360 - _pinball_loss: 1.7377 - val_loss: 2.0886 - val_mean_absolute_error: 8.6988 - val_mean_squared_error: 314.0332 - val__pinball_loss: 2.0886\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.6388 - mean_absolute_error: 6.9660 - mean_squared_error: 218.1192 - _pinball_loss: 1.6388 - val_loss: 2.0116 - val_mean_absolute_error: 8.3140 - val_mean_squared_error: 302.8333 - val__pinball_loss: 2.0116\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.5843 - mean_absolute_error: 6.7799 - mean_squared_error: 214.8402 - _pinball_loss: 1.5843 - val_loss: 1.9766 - val_mean_absolute_error: 8.5009 - val_mean_squared_error: 313.0850 - val__pinball_loss: 1.9766\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.5530 - mean_absolute_error: 6.6827 - mean_squared_error: 213.3719 - _pinball_loss: 1.5530 - val_loss: 1.9590 - val_mean_absolute_error: 8.4311 - val_mean_squared_error: 309.9476 - val__pinball_loss: 1.9590\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.5315 - mean_absolute_error: 6.6162 - mean_squared_error: 212.3037 - _pinball_loss: 1.5315 - val_loss: 1.9670 - val_mean_absolute_error: 8.5549 - val_mean_squared_error: 315.3344 - val__pinball_loss: 1.9670\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.5113 - mean_absolute_error: 6.5535 - mean_squared_error: 211.0037 - _pinball_loss: 1.5113 - val_loss: 1.9362 - val_mean_absolute_error: 8.4718 - val_mean_squared_error: 312.6653 - val__pinball_loss: 1.9362\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.4990 - mean_absolute_error: 6.5183 - mean_squared_error: 210.5427 - _pinball_loss: 1.4990 - val_loss: 1.9132 - val_mean_absolute_error: 8.3246 - val_mean_squared_error: 307.5745 - val__pinball_loss: 1.9132\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.4813 - mean_absolute_error: 6.4628 - mean_squared_error: 209.6226 - _pinball_loss: 1.4813 - val_loss: 1.9076 - val_mean_absolute_error: 8.1253 - val_mean_squared_error: 297.7860 - val__pinball_loss: 1.9076\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4711 - mean_absolute_error: 6.4314 - mean_squared_error: 208.7052 - _pinball_loss: 1.4711 - val_loss: 1.8989 - val_mean_absolute_error: 8.2669 - val_mean_squared_error: 309.1449 - val__pinball_loss: 1.8989\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 11s 9ms/step - loss: 1.4568 - mean_absolute_error: 6.3891 - mean_squared_error: 208.2199 - _pinball_loss: 1.4568 - val_loss: 1.8979 - val_mean_absolute_error: 8.2213 - val_mean_squared_error: 303.4959 - val__pinball_loss: 1.8979\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4466 - mean_absolute_error: 6.3671 - mean_squared_error: 208.0264 - _pinball_loss: 1.4466 - val_loss: 1.8890 - val_mean_absolute_error: 7.8126 - val_mean_squared_error: 289.4972 - val__pinball_loss: 1.8890\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4376 - mean_absolute_error: 6.3323 - mean_squared_error: 207.1269 - _pinball_loss: 1.4376 - val_loss: 1.8709 - val_mean_absolute_error: 7.9860 - val_mean_squared_error: 297.9732 - val__pinball_loss: 1.8709\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4245 - mean_absolute_error: 6.2912 - mean_squared_error: 206.3157 - _pinball_loss: 1.4245 - val_loss: 1.8647 - val_mean_absolute_error: 8.0831 - val_mean_squared_error: 302.3588 - val__pinball_loss: 1.8647\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4193 - mean_absolute_error: 6.2804 - mean_squared_error: 206.2252 - _pinball_loss: 1.4193 - val_loss: 1.8869 - val_mean_absolute_error: 8.1675 - val_mean_squared_error: 301.5304 - val__pinball_loss: 1.8869\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4122 - mean_absolute_error: 6.2610 - mean_squared_error: 205.5728 - _pinball_loss: 1.4122 - val_loss: 1.8613 - val_mean_absolute_error: 8.3989 - val_mean_squared_error: 317.5504 - val__pinball_loss: 1.8613\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.4003 - mean_absolute_error: 6.2234 - mean_squared_error: 205.0184 - _pinball_loss: 1.4003 - val_loss: 1.8512 - val_mean_absolute_error: 7.9332 - val_mean_squared_error: 296.0874 - val__pinball_loss: 1.8512\n",
      "Epoch 18/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3955 - mean_absolute_error: 6.2080 - mean_squared_error: 204.2452 - _pinball_loss: 1.3955 - val_loss: 1.8501 - val_mean_absolute_error: 7.9284 - val_mean_squared_error: 299.9467 - val__pinball_loss: 1.8501\n",
      "Epoch 19/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3861 - mean_absolute_error: 6.1773 - mean_squared_error: 203.6210 - _pinball_loss: 1.3861 - val_loss: 1.8419 - val_mean_absolute_error: 8.0586 - val_mean_squared_error: 305.4525 - val__pinball_loss: 1.8419\n",
      "Epoch 20/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3791 - mean_absolute_error: 6.1541 - mean_squared_error: 202.8859 - _pinball_loss: 1.3791 - val_loss: 1.8478 - val_mean_absolute_error: 8.1644 - val_mean_squared_error: 308.3103 - val__pinball_loss: 1.8478\n",
      "Epoch 21/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3742 - mean_absolute_error: 6.1365 - mean_squared_error: 202.2661 - _pinball_loss: 1.3742 - val_loss: 1.8327 - val_mean_absolute_error: 8.0316 - val_mean_squared_error: 302.6826 - val__pinball_loss: 1.8327\n",
      "Epoch 22/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3673 - mean_absolute_error: 6.1181 - mean_squared_error: 201.7806 - _pinball_loss: 1.3673 - val_loss: 1.8324 - val_mean_absolute_error: 7.9899 - val_mean_squared_error: 304.7495 - val__pinball_loss: 1.8324\n",
      "Epoch 23/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3600 - mean_absolute_error: 6.0905 - mean_squared_error: 201.0398 - _pinball_loss: 1.3600 - val_loss: 1.8414 - val_mean_absolute_error: 7.7649 - val_mean_squared_error: 290.5173 - val__pinball_loss: 1.8414\n",
      "Epoch 24/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3572 - mean_absolute_error: 6.0781 - mean_squared_error: 200.6256 - _pinball_loss: 1.3572 - val_loss: 1.8342 - val_mean_absolute_error: 8.2042 - val_mean_squared_error: 309.7202 - val__pinball_loss: 1.8342\n",
      "Epoch 25/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3484 - mean_absolute_error: 6.0522 - mean_squared_error: 199.9150 - _pinball_loss: 1.3484 - val_loss: 1.8245 - val_mean_absolute_error: 7.9683 - val_mean_squared_error: 303.1305 - val__pinball_loss: 1.8245\n",
      "Epoch 26/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3439 - mean_absolute_error: 6.0384 - mean_squared_error: 199.5312 - _pinball_loss: 1.3439 - val_loss: 1.8652 - val_mean_absolute_error: 7.7355 - val_mean_squared_error: 290.3091 - val__pinball_loss: 1.8652\n",
      "Epoch 27/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3417 - mean_absolute_error: 6.0245 - mean_squared_error: 198.8737 - _pinball_loss: 1.3417 - val_loss: 1.8217 - val_mean_absolute_error: 7.9182 - val_mean_squared_error: 300.7410 - val__pinball_loss: 1.8217\n",
      "Epoch 28/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3325 - mean_absolute_error: 6.0012 - mean_squared_error: 198.4421 - _pinball_loss: 1.3325 - val_loss: 1.8317 - val_mean_absolute_error: 7.8336 - val_mean_squared_error: 295.0376 - val__pinball_loss: 1.8317\n",
      "Epoch 29/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3251 - mean_absolute_error: 5.9739 - mean_squared_error: 197.5930 - _pinball_loss: 1.3251 - val_loss: 1.8353 - val_mean_absolute_error: 7.7435 - val_mean_squared_error: 290.3958 - val__pinball_loss: 1.8353\n",
      "Epoch 30/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.3229 - mean_absolute_error: 5.9644 - mean_squared_error: 197.0519 - _pinball_loss: 1.3229 - val_loss: 1.8452 - val_mean_absolute_error: 8.2547 - val_mean_squared_error: 315.3579 - val__pinball_loss: 1.8452\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FEB39B8828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.9\n",
      "Epoch 1/1000\n",
      "1137/1137 [==============================] - 12s 10ms/step - loss: 3.9054 - mean_absolute_error: 19.3561 - mean_squared_error: 762.4905 - _pinball_loss: 3.9054 - val_loss: 1.3846 - val_mean_absolute_error: 11.1609 - val_mean_squared_error: 398.9000 - val__pinball_loss: 1.3846\n",
      "Epoch 2/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.1085 - mean_absolute_error: 9.3806 - mean_squared_error: 284.0353 - _pinball_loss: 1.1085 - val_loss: 1.2863 - val_mean_absolute_error: 11.2759 - val_mean_squared_error: 405.0883 - val__pinball_loss: 1.2863\n",
      "Epoch 3/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 1.0288 - mean_absolute_error: 8.8683 - mean_squared_error: 278.0606 - _pinball_loss: 1.0288 - val_loss: 1.2374 - val_mean_absolute_error: 10.7219 - val_mean_squared_error: 394.3552 - val__pinball_loss: 1.2374\n",
      "Epoch 4/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.9876 - mean_absolute_error: 8.5286 - mean_squared_error: 271.8616 - _pinball_loss: 0.9876 - val_loss: 1.2200 - val_mean_absolute_error: 10.3824 - val_mean_squared_error: 382.0524 - val__pinball_loss: 1.2200\n",
      "Epoch 5/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.9573 - mean_absolute_error: 8.2756 - mean_squared_error: 266.2992 - _pinball_loss: 0.9573 - val_loss: 1.1978 - val_mean_absolute_error: 10.4623 - val_mean_squared_error: 389.2110 - val__pinball_loss: 1.1978\n",
      "Epoch 6/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.9323 - mean_absolute_error: 8.0900 - mean_squared_error: 262.5513 - _pinball_loss: 0.9323 - val_loss: 1.1668 - val_mean_absolute_error: 9.9556 - val_mean_squared_error: 369.3618 - val__pinball_loss: 1.1668\n",
      "Epoch 7/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.9130 - mean_absolute_error: 7.9483 - mean_squared_error: 259.5556 - _pinball_loss: 0.9130 - val_loss: 1.1593 - val_mean_absolute_error: 9.7984 - val_mean_squared_error: 365.9059 - val__pinball_loss: 1.1593\n",
      "Epoch 8/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8992 - mean_absolute_error: 7.8472 - mean_squared_error: 256.9773 - _pinball_loss: 0.8992 - val_loss: 1.1608 - val_mean_absolute_error: 10.0568 - val_mean_squared_error: 375.6268 - val__pinball_loss: 1.1608\n",
      "Epoch 9/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8882 - mean_absolute_error: 7.7683 - mean_squared_error: 255.0204 - _pinball_loss: 0.8882 - val_loss: 1.1364 - val_mean_absolute_error: 9.5188 - val_mean_squared_error: 361.8074 - val__pinball_loss: 1.1364\n",
      "Epoch 10/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8774 - mean_absolute_error: 7.6930 - mean_squared_error: 253.1253 - _pinball_loss: 0.8774 - val_loss: 1.1364 - val_mean_absolute_error: 9.7011 - val_mean_squared_error: 367.2108 - val__pinball_loss: 1.1364\n",
      "Epoch 11/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8701 - mean_absolute_error: 7.6419 - mean_squared_error: 251.7507 - _pinball_loss: 0.8701 - val_loss: 1.1275 - val_mean_absolute_error: 9.8113 - val_mean_squared_error: 371.5433 - val__pinball_loss: 1.1275\n",
      "Epoch 12/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8635 - mean_absolute_error: 7.6062 - mean_squared_error: 250.7862 - _pinball_loss: 0.8635 - val_loss: 1.1162 - val_mean_absolute_error: 9.4446 - val_mean_squared_error: 357.6279 - val__pinball_loss: 1.1162\n",
      "Epoch 13/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8544 - mean_absolute_error: 7.5352 - mean_squared_error: 248.7036 - _pinball_loss: 0.8544 - val_loss: 1.1047 - val_mean_absolute_error: 9.4177 - val_mean_squared_error: 356.8220 - val__pinball_loss: 1.1047\n",
      "Epoch 14/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8457 - mean_absolute_error: 7.4734 - mean_squared_error: 247.0816 - _pinball_loss: 0.8457 - val_loss: 1.1109 - val_mean_absolute_error: 9.6966 - val_mean_squared_error: 366.8552 - val__pinball_loss: 1.1109\n",
      "Epoch 15/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8414 - mean_absolute_error: 7.4449 - mean_squared_error: 245.9620 - _pinball_loss: 0.8414 - val_loss: 1.0938 - val_mean_absolute_error: 9.1980 - val_mean_squared_error: 350.9301 - val__pinball_loss: 1.0938\n",
      "Epoch 16/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8355 - mean_absolute_error: 7.4042 - mean_squared_error: 245.0702 - _pinball_loss: 0.8355 - val_loss: 1.0960 - val_mean_absolute_error: 9.3838 - val_mean_squared_error: 360.3766 - val__pinball_loss: 1.0960\n",
      "Epoch 17/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8296 - mean_absolute_error: 7.3705 - mean_squared_error: 244.3639 - _pinball_loss: 0.8296 - val_loss: 1.0799 - val_mean_absolute_error: 9.4809 - val_mean_squared_error: 362.4169 - val__pinball_loss: 1.0799\n",
      "Epoch 18/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8236 - mean_absolute_error: 7.3342 - mean_squared_error: 243.1613 - _pinball_loss: 0.8236 - val_loss: 1.0777 - val_mean_absolute_error: 9.3317 - val_mean_squared_error: 356.1844 - val__pinball_loss: 1.0777\n",
      "Epoch 19/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8176 - mean_absolute_error: 7.2919 - mean_squared_error: 242.1192 - _pinball_loss: 0.8176 - val_loss: 1.0713 - val_mean_absolute_error: 9.3738 - val_mean_squared_error: 359.4464 - val__pinball_loss: 1.0713\n",
      "Epoch 20/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8118 - mean_absolute_error: 7.2607 - mean_squared_error: 241.2535 - _pinball_loss: 0.8118 - val_loss: 1.0698 - val_mean_absolute_error: 9.4485 - val_mean_squared_error: 358.4205 - val__pinball_loss: 1.0698\n",
      "Epoch 21/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8076 - mean_absolute_error: 7.2353 - mean_squared_error: 240.3004 - _pinball_loss: 0.8076 - val_loss: 1.0608 - val_mean_absolute_error: 9.1854 - val_mean_squared_error: 345.1282 - val__pinball_loss: 1.0608\n",
      "Epoch 22/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.8034 - mean_absolute_error: 7.2056 - mean_squared_error: 239.4410 - _pinball_loss: 0.8034 - val_loss: 1.0558 - val_mean_absolute_error: 9.2131 - val_mean_squared_error: 350.6462 - val__pinball_loss: 1.0558\n",
      "Epoch 23/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7980 - mean_absolute_error: 7.1771 - mean_squared_error: 238.8774 - _pinball_loss: 0.7980 - val_loss: 1.0604 - val_mean_absolute_error: 8.7745 - val_mean_squared_error: 331.5421 - val__pinball_loss: 1.0604\n",
      "Epoch 24/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7935 - mean_absolute_error: 7.1395 - mean_squared_error: 237.2519 - _pinball_loss: 0.7935 - val_loss: 1.0553 - val_mean_absolute_error: 9.0638 - val_mean_squared_error: 344.5575 - val__pinball_loss: 1.0553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7880 - mean_absolute_error: 7.1121 - mean_squared_error: 236.9266 - _pinball_loss: 0.7880 - val_loss: 1.0560 - val_mean_absolute_error: 8.7284 - val_mean_squared_error: 331.8808 - val__pinball_loss: 1.0560\n",
      "Epoch 26/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7837 - mean_absolute_error: 7.0824 - mean_squared_error: 236.1302 - _pinball_loss: 0.7837 - val_loss: 1.0501 - val_mean_absolute_error: 8.9753 - val_mean_squared_error: 341.0547 - val__pinball_loss: 1.0501\n",
      "Epoch 27/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7797 - mean_absolute_error: 7.0597 - mean_squared_error: 235.4641 - _pinball_loss: 0.7797 - val_loss: 1.0513 - val_mean_absolute_error: 8.9891 - val_mean_squared_error: 347.2086 - val__pinball_loss: 1.0513\n",
      "Epoch 28/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7768 - mean_absolute_error: 7.0392 - mean_squared_error: 234.6892 - _pinball_loss: 0.7768 - val_loss: 1.0429 - val_mean_absolute_error: 9.1305 - val_mean_squared_error: 349.6727 - val__pinball_loss: 1.0429\n",
      "Epoch 29/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7745 - mean_absolute_error: 7.0325 - mean_squared_error: 234.3320 - _pinball_loss: 0.7745 - val_loss: 1.0465 - val_mean_absolute_error: 9.1378 - val_mean_squared_error: 346.2750 - val__pinball_loss: 1.0465\n",
      "Epoch 30/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7696 - mean_absolute_error: 6.9898 - mean_squared_error: 233.1114 - _pinball_loss: 0.7696 - val_loss: 1.0457 - val_mean_absolute_error: 8.9650 - val_mean_squared_error: 339.7377 - val__pinball_loss: 1.0457\n",
      "Epoch 31/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7662 - mean_absolute_error: 6.9655 - mean_squared_error: 232.1391 - _pinball_loss: 0.7662 - val_loss: 1.0398 - val_mean_absolute_error: 9.1542 - val_mean_squared_error: 348.8222 - val__pinball_loss: 1.0398\n",
      "Epoch 32/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7643 - mean_absolute_error: 6.9567 - mean_squared_error: 231.4500 - _pinball_loss: 0.7643 - val_loss: 1.0624 - val_mean_absolute_error: 8.6614 - val_mean_squared_error: 328.4940 - val__pinball_loss: 1.0624\n",
      "Epoch 33/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7576 - mean_absolute_error: 6.9044 - mean_squared_error: 230.4586 - _pinball_loss: 0.7576 - val_loss: 1.0357 - val_mean_absolute_error: 8.8298 - val_mean_squared_error: 339.2905 - val__pinball_loss: 1.0357\n",
      "Epoch 34/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7562 - mean_absolute_error: 6.9013 - mean_squared_error: 230.2550 - _pinball_loss: 0.7562 - val_loss: 1.0489 - val_mean_absolute_error: 8.5760 - val_mean_squared_error: 327.3043 - val__pinball_loss: 1.0489\n",
      "Epoch 35/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7536 - mean_absolute_error: 6.8761 - mean_squared_error: 229.0475 - _pinball_loss: 0.7536 - val_loss: 1.0487 - val_mean_absolute_error: 8.5980 - val_mean_squared_error: 329.5851 - val__pinball_loss: 1.0487\n",
      "Epoch 36/1000\n",
      "1137/1137 [==============================] - 11s 10ms/step - loss: 0.7477 - mean_absolute_error: 6.8339 - mean_squared_error: 228.4305 - _pinball_loss: 0.7477 - val_loss: 1.0506 - val_mean_absolute_error: 8.6989 - val_mean_squared_error: 330.9355 - val__pinball_loss: 1.0506\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000200F38F7A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import src.model.multiple_output.convolution\n",
    "import importlib\n",
    "importlib.reload(src.model.multiple_output.convolution)\n",
    "\n",
    "\n",
    "days = 7\n",
    "\n",
    "one_days_window_label_columns = WindowGenerator(\n",
    "    train_df,\n",
    "    valid_df,\n",
    "    test_df,\n",
    "    input_width=48 * days,\n",
    "    label_width=96,\n",
    "    shift=96,\n",
    "    sequence_stride=1,\n",
    "    label_columns=[\"TARGET\"]\n",
    "    \n",
    ")\n",
    "submission_df = load_submission_data()\n",
    "_, predict_df = load_basic_preprocessed_predict(\"minmax\")\n",
    "predict_df[\"scaled_TARGET\"] = predict_df[\"TARGET\"]\n",
    "predict_df = predict_df[cutter]\n",
    "predict_df.drop(\"TARGET\", axis=1, inplace=True)\n",
    "predict_df = load_test_features(predict_df, 48 * days)\n",
    "\n",
    "evaluate_dict = {}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    q = i/10\n",
    "    print(q)\n",
    "    \n",
    "    conv_various = Convolution2DVarious(days, 96, train_df.shape[-1] - 1)\n",
    "\n",
    "    compile_and_fit_with_pinball_loss(conv_various, one_days_window_label_columns, q)\n",
    "    evaluate_dict[q] = conv_various.evaluate(one_days_window_label_columns.test, verbose=0)\n",
    "    predict_np = predict_df.reshape(-1, days, 48, train_df.shape[-1] - 1)\n",
    "    pred_y = conv_various.predict(predict_np)[:, :, -1]\n",
    "    submission_df[f\"q_{q}\"] = pred_y.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: [0.9796475768089294,\n",
       "  7.050142288208008,\n",
       "  181.86700439453125,\n",
       "  0.9796915650367737],\n",
       " 0.2: [1.5727002620697021,\n",
       "  5.466338634490967,\n",
       "  110.00485229492188,\n",
       "  1.5729024410247803],\n",
       " 0.3: [1.845992922782898,\n",
       "  4.098628997802734,\n",
       "  70.45992279052734,\n",
       "  1.8464171886444092],\n",
       " 0.4: [1.850978136062622,\n",
       "  3.6372554302215576,\n",
       "  66.23145294189453,\n",
       "  1.8508955240249634],\n",
       " 0.5: [1.779157042503357,\n",
       "  3.5583150386810303,\n",
       "  69.03361511230469,\n",
       "  1.7789722681045532],\n",
       " 0.6: [1.5520992279052734,\n",
       "  3.5552873611450195,\n",
       "  81.22003173828125,\n",
       "  1.5518927574157715],\n",
       " 0.7: [1.3264968395233154,\n",
       "  4.085760593414307,\n",
       "  103.8653564453125,\n",
       "  1.3263262510299683],\n",
       " 0.8: [0.961068332195282,\n",
       "  4.501470565795898,\n",
       "  121.0285873413086,\n",
       "  0.96099853515625],\n",
       " 0.9: [0.5296670794487,\n",
       "  4.820670127868652,\n",
       "  125.06450653076172,\n",
       "  0.5297747254371643]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3775341577000089"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for v in evaluate_dict.values():\n",
    "    s += v[0]\n",
    "\n",
    "s/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>-0.032475</td>\n",
       "      <td>-0.105103</td>\n",
       "      <td>-0.098774</td>\n",
       "      <td>0.063773</td>\n",
       "      <td>-0.027927</td>\n",
       "      <td>-0.022728</td>\n",
       "      <td>-0.108168</td>\n",
       "      <td>0.074562</td>\n",
       "      <td>0.595425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>-0.025982</td>\n",
       "      <td>0.013037</td>\n",
       "      <td>-0.228631</td>\n",
       "      <td>0.023997</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.082372</td>\n",
       "      <td>-0.084517</td>\n",
       "      <td>-0.059915</td>\n",
       "      <td>1.063461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>-0.047746</td>\n",
       "      <td>-0.016477</td>\n",
       "      <td>-0.366813</td>\n",
       "      <td>-0.010404</td>\n",
       "      <td>-0.088813</td>\n",
       "      <td>0.032507</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>1.504606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>-0.045246</td>\n",
       "      <td>-0.011987</td>\n",
       "      <td>-0.087656</td>\n",
       "      <td>0.076161</td>\n",
       "      <td>0.061755</td>\n",
       "      <td>0.199537</td>\n",
       "      <td>0.171587</td>\n",
       "      <td>0.039975</td>\n",
       "      <td>1.511725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>-0.037601</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>-0.113075</td>\n",
       "      <td>-0.053915</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>0.453990</td>\n",
       "      <td>0.407158</td>\n",
       "      <td>0.145288</td>\n",
       "      <td>1.324745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>-0.028831</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>-0.070114</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.295865</td>\n",
       "      <td>0.312074</td>\n",
       "      <td>0.099051</td>\n",
       "      <td>0.968625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>-0.011036</td>\n",
       "      <td>-0.076511</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.151301</td>\n",
       "      <td>0.406871</td>\n",
       "      <td>0.055930</td>\n",
       "      <td>0.957362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>-0.038073</td>\n",
       "      <td>-0.029464</td>\n",
       "      <td>-0.327148</td>\n",
       "      <td>-0.026507</td>\n",
       "      <td>-0.012956</td>\n",
       "      <td>-0.038526</td>\n",
       "      <td>0.247233</td>\n",
       "      <td>0.030798</td>\n",
       "      <td>0.219046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>-0.038016</td>\n",
       "      <td>-0.060692</td>\n",
       "      <td>-0.689418</td>\n",
       "      <td>0.036708</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>-0.103541</td>\n",
       "      <td>0.319506</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.495112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>-0.033922</td>\n",
       "      <td>-0.113797</td>\n",
       "      <td>-1.062824</td>\n",
       "      <td>0.134667</td>\n",
       "      <td>0.109588</td>\n",
       "      <td>0.042954</td>\n",
       "      <td>0.484153</td>\n",
       "      <td>0.431663</td>\n",
       "      <td>0.559823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>-0.044573</td>\n",
       "      <td>-0.198870</td>\n",
       "      <td>-1.418363</td>\n",
       "      <td>0.258881</td>\n",
       "      <td>0.202406</td>\n",
       "      <td>0.295156</td>\n",
       "      <td>0.441591</td>\n",
       "      <td>0.528161</td>\n",
       "      <td>-0.087109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>-0.230250</td>\n",
       "      <td>-0.577085</td>\n",
       "      <td>-1.719793</td>\n",
       "      <td>0.095863</td>\n",
       "      <td>0.194881</td>\n",
       "      <td>0.324341</td>\n",
       "      <td>0.190489</td>\n",
       "      <td>0.708684</td>\n",
       "      <td>0.617961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>-0.156363</td>\n",
       "      <td>-0.289281</td>\n",
       "      <td>-1.788932</td>\n",
       "      <td>-0.015102</td>\n",
       "      <td>0.049515</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.089325</td>\n",
       "      <td>1.246120</td>\n",
       "      <td>1.621673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>-0.097249</td>\n",
       "      <td>0.032490</td>\n",
       "      <td>-1.394271</td>\n",
       "      <td>-0.008496</td>\n",
       "      <td>-0.004574</td>\n",
       "      <td>-0.053907</td>\n",
       "      <td>0.441158</td>\n",
       "      <td>1.455138</td>\n",
       "      <td>3.089519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.030976</td>\n",
       "      <td>0.388660</td>\n",
       "      <td>-0.214347</td>\n",
       "      <td>0.231003</td>\n",
       "      <td>-0.022536</td>\n",
       "      <td>0.591490</td>\n",
       "      <td>1.744735</td>\n",
       "      <td>2.592998</td>\n",
       "      <td>5.093650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>1.145264</td>\n",
       "      <td>1.771394</td>\n",
       "      <td>0.622307</td>\n",
       "      <td>1.797454</td>\n",
       "      <td>1.238429</td>\n",
       "      <td>1.915306</td>\n",
       "      <td>4.540763</td>\n",
       "      <td>5.262686</td>\n",
       "      <td>8.937643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>2.383642</td>\n",
       "      <td>3.587969</td>\n",
       "      <td>2.915865</td>\n",
       "      <td>4.347979</td>\n",
       "      <td>4.637251</td>\n",
       "      <td>4.823846</td>\n",
       "      <td>8.629528</td>\n",
       "      <td>9.494079</td>\n",
       "      <td>13.571690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>3.891693</td>\n",
       "      <td>6.636632</td>\n",
       "      <td>5.512778</td>\n",
       "      <td>7.801158</td>\n",
       "      <td>8.834262</td>\n",
       "      <td>9.745342</td>\n",
       "      <td>14.793843</td>\n",
       "      <td>15.650631</td>\n",
       "      <td>19.707041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.920645</td>\n",
       "      <td>10.082118</td>\n",
       "      <td>8.638920</td>\n",
       "      <td>11.743272</td>\n",
       "      <td>13.582507</td>\n",
       "      <td>15.869155</td>\n",
       "      <td>20.750513</td>\n",
       "      <td>22.549725</td>\n",
       "      <td>26.517036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>7.578732</td>\n",
       "      <td>12.495900</td>\n",
       "      <td>13.091483</td>\n",
       "      <td>15.564502</td>\n",
       "      <td>18.910107</td>\n",
       "      <td>21.792852</td>\n",
       "      <td>27.005182</td>\n",
       "      <td>29.240368</td>\n",
       "      <td>34.093887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>9.202325</td>\n",
       "      <td>14.370083</td>\n",
       "      <td>17.380587</td>\n",
       "      <td>18.922745</td>\n",
       "      <td>21.778227</td>\n",
       "      <td>26.887941</td>\n",
       "      <td>33.066570</td>\n",
       "      <td>35.087997</td>\n",
       "      <td>40.060902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>10.377932</td>\n",
       "      <td>16.560617</td>\n",
       "      <td>21.049261</td>\n",
       "      <td>22.855680</td>\n",
       "      <td>26.007191</td>\n",
       "      <td>30.495989</td>\n",
       "      <td>37.841412</td>\n",
       "      <td>39.055458</td>\n",
       "      <td>44.992802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>11.315662</td>\n",
       "      <td>17.851332</td>\n",
       "      <td>23.594387</td>\n",
       "      <td>26.603224</td>\n",
       "      <td>29.110861</td>\n",
       "      <td>32.556499</td>\n",
       "      <td>41.089462</td>\n",
       "      <td>41.923168</td>\n",
       "      <td>48.452892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>11.923900</td>\n",
       "      <td>18.096512</td>\n",
       "      <td>25.127829</td>\n",
       "      <td>28.512575</td>\n",
       "      <td>31.546825</td>\n",
       "      <td>35.788204</td>\n",
       "      <td>43.759438</td>\n",
       "      <td>43.584980</td>\n",
       "      <td>49.854446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>12.259184</td>\n",
       "      <td>17.972837</td>\n",
       "      <td>24.602896</td>\n",
       "      <td>29.206108</td>\n",
       "      <td>32.086597</td>\n",
       "      <td>37.434673</td>\n",
       "      <td>44.159756</td>\n",
       "      <td>44.015755</td>\n",
       "      <td>50.155651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>11.664116</td>\n",
       "      <td>16.960072</td>\n",
       "      <td>23.584049</td>\n",
       "      <td>27.749851</td>\n",
       "      <td>31.522541</td>\n",
       "      <td>36.887539</td>\n",
       "      <td>44.263329</td>\n",
       "      <td>43.224564</td>\n",
       "      <td>48.869244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>10.191174</td>\n",
       "      <td>16.064186</td>\n",
       "      <td>22.387798</td>\n",
       "      <td>25.843534</td>\n",
       "      <td>29.914156</td>\n",
       "      <td>35.671467</td>\n",
       "      <td>42.116119</td>\n",
       "      <td>40.326588</td>\n",
       "      <td>46.095654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>8.338793</td>\n",
       "      <td>14.031995</td>\n",
       "      <td>20.028107</td>\n",
       "      <td>23.580259</td>\n",
       "      <td>26.111427</td>\n",
       "      <td>33.341022</td>\n",
       "      <td>38.915386</td>\n",
       "      <td>36.632172</td>\n",
       "      <td>42.542233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>6.438894</td>\n",
       "      <td>11.798625</td>\n",
       "      <td>17.772745</td>\n",
       "      <td>20.204332</td>\n",
       "      <td>22.745476</td>\n",
       "      <td>30.142757</td>\n",
       "      <td>34.384727</td>\n",
       "      <td>32.552292</td>\n",
       "      <td>37.896477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>4.642674</td>\n",
       "      <td>8.917039</td>\n",
       "      <td>14.868993</td>\n",
       "      <td>16.636839</td>\n",
       "      <td>19.511023</td>\n",
       "      <td>25.442408</td>\n",
       "      <td>28.204422</td>\n",
       "      <td>27.932016</td>\n",
       "      <td>32.630417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>2.930609</td>\n",
       "      <td>6.400648</td>\n",
       "      <td>11.751292</td>\n",
       "      <td>12.728077</td>\n",
       "      <td>15.909344</td>\n",
       "      <td>19.699924</td>\n",
       "      <td>20.741110</td>\n",
       "      <td>21.636147</td>\n",
       "      <td>25.989923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>1.649050</td>\n",
       "      <td>3.933033</td>\n",
       "      <td>8.617319</td>\n",
       "      <td>8.952081</td>\n",
       "      <td>11.080450</td>\n",
       "      <td>13.816643</td>\n",
       "      <td>14.024981</td>\n",
       "      <td>15.228221</td>\n",
       "      <td>19.284466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>1.807476</td>\n",
       "      <td>5.380528</td>\n",
       "      <td>5.751849</td>\n",
       "      <td>5.436456</td>\n",
       "      <td>7.641447</td>\n",
       "      <td>8.822656</td>\n",
       "      <td>9.219543</td>\n",
       "      <td>12.351213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>0.201524</td>\n",
       "      <td>0.623902</td>\n",
       "      <td>3.130742</td>\n",
       "      <td>2.934508</td>\n",
       "      <td>1.469816</td>\n",
       "      <td>3.469676</td>\n",
       "      <td>5.192949</td>\n",
       "      <td>5.178250</td>\n",
       "      <td>7.116824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>-0.015940</td>\n",
       "      <td>-0.053902</td>\n",
       "      <td>1.742528</td>\n",
       "      <td>0.971621</td>\n",
       "      <td>0.092404</td>\n",
       "      <td>0.868362</td>\n",
       "      <td>2.794237</td>\n",
       "      <td>3.109246</td>\n",
       "      <td>3.915729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>-0.043710</td>\n",
       "      <td>-0.165341</td>\n",
       "      <td>1.029288</td>\n",
       "      <td>0.069415</td>\n",
       "      <td>-0.352298</td>\n",
       "      <td>-0.082172</td>\n",
       "      <td>1.894108</td>\n",
       "      <td>2.155954</td>\n",
       "      <td>2.454201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>-0.040556</td>\n",
       "      <td>-0.103839</td>\n",
       "      <td>0.429723</td>\n",
       "      <td>-0.155786</td>\n",
       "      <td>-0.193194</td>\n",
       "      <td>-0.187773</td>\n",
       "      <td>1.143442</td>\n",
       "      <td>1.366849</td>\n",
       "      <td>1.423467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>-0.057124</td>\n",
       "      <td>-0.022649</td>\n",
       "      <td>0.096232</td>\n",
       "      <td>-0.367589</td>\n",
       "      <td>-0.001791</td>\n",
       "      <td>-0.059490</td>\n",
       "      <td>1.029982</td>\n",
       "      <td>0.804624</td>\n",
       "      <td>0.757860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>-0.036151</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.154644</td>\n",
       "      <td>-0.245674</td>\n",
       "      <td>-0.094616</td>\n",
       "      <td>-0.124035</td>\n",
       "      <td>0.505330</td>\n",
       "      <td>0.283033</td>\n",
       "      <td>0.461291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>0.039934</td>\n",
       "      <td>-0.078590</td>\n",
       "      <td>-0.166324</td>\n",
       "      <td>-0.043196</td>\n",
       "      <td>-0.143338</td>\n",
       "      <td>0.338303</td>\n",
       "      <td>-0.070067</td>\n",
       "      <td>0.242706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>-0.039703</td>\n",
       "      <td>-0.014718</td>\n",
       "      <td>0.026451</td>\n",
       "      <td>-0.170821</td>\n",
       "      <td>-0.066030</td>\n",
       "      <td>-0.101935</td>\n",
       "      <td>0.066204</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.373869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>-0.040474</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>-0.007638</td>\n",
       "      <td>-0.075785</td>\n",
       "      <td>-0.143889</td>\n",
       "      <td>-0.159693</td>\n",
       "      <td>-0.094389</td>\n",
       "      <td>-0.194270</td>\n",
       "      <td>0.490138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>-0.044599</td>\n",
       "      <td>0.031964</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>0.114666</td>\n",
       "      <td>-0.047797</td>\n",
       "      <td>0.169868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>-0.040397</td>\n",
       "      <td>0.017927</td>\n",
       "      <td>-0.026830</td>\n",
       "      <td>-0.018662</td>\n",
       "      <td>-0.084809</td>\n",
       "      <td>-0.035200</td>\n",
       "      <td>0.142241</td>\n",
       "      <td>-0.186230</td>\n",
       "      <td>0.260162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>-0.021974</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>-0.078171</td>\n",
       "      <td>-0.085914</td>\n",
       "      <td>-0.109154</td>\n",
       "      <td>-0.090836</td>\n",
       "      <td>-0.059531</td>\n",
       "      <td>-0.018495</td>\n",
       "      <td>0.028261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>-0.031366</td>\n",
       "      <td>-0.034161</td>\n",
       "      <td>-0.092709</td>\n",
       "      <td>-0.065930</td>\n",
       "      <td>-0.013743</td>\n",
       "      <td>0.116565</td>\n",
       "      <td>0.130139</td>\n",
       "      <td>-0.095092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>-0.090818</td>\n",
       "      <td>-0.108225</td>\n",
       "      <td>-0.112969</td>\n",
       "      <td>-0.039415</td>\n",
       "      <td>-0.071323</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.177514</td>\n",
       "      <td>-0.042162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>-0.010059</td>\n",
       "      <td>-0.039430</td>\n",
       "      <td>-0.084664</td>\n",
       "      <td>-0.129694</td>\n",
       "      <td>-0.097498</td>\n",
       "      <td>-0.092663</td>\n",
       "      <td>0.146892</td>\n",
       "      <td>0.271685</td>\n",
       "      <td>0.165213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m  -0.032475  -0.105103  -0.098774   0.063773  -0.027927   \n",
       "1    0.csv_Day7_0h30m  -0.025982   0.013037  -0.228631   0.023997   0.006828   \n",
       "2    0.csv_Day7_1h00m  -0.047746  -0.016477  -0.366813  -0.010404  -0.088813   \n",
       "3    0.csv_Day7_1h30m  -0.045246  -0.011987  -0.087656   0.076161   0.061755   \n",
       "4    0.csv_Day7_2h00m  -0.037601   0.000744  -0.113075  -0.053915   0.035461   \n",
       "5    0.csv_Day7_2h30m  -0.028831   0.000839  -0.070114  -0.067453   0.020227   \n",
       "6    0.csv_Day7_3h00m   0.002505   0.028633  -0.011036  -0.076511   0.148333   \n",
       "7    0.csv_Day7_3h30m  -0.038073  -0.029464  -0.327148  -0.026507  -0.012956   \n",
       "8    0.csv_Day7_4h00m  -0.038016  -0.060692  -0.689418   0.036708   0.017185   \n",
       "9    0.csv_Day7_4h30m  -0.033922  -0.113797  -1.062824   0.134667   0.109588   \n",
       "10   0.csv_Day7_5h00m  -0.044573  -0.198870  -1.418363   0.258881   0.202406   \n",
       "11   0.csv_Day7_5h30m  -0.230250  -0.577085  -1.719793   0.095863   0.194881   \n",
       "12   0.csv_Day7_6h00m  -0.156363  -0.289281  -1.788932  -0.015102   0.049515   \n",
       "13   0.csv_Day7_6h30m  -0.097249   0.032490  -1.394271  -0.008496  -0.004574   \n",
       "14   0.csv_Day7_7h00m   0.030976   0.388660  -0.214347   0.231003  -0.022536   \n",
       "15   0.csv_Day7_7h30m   1.145264   1.771394   0.622307   1.797454   1.238429   \n",
       "16   0.csv_Day7_8h00m   2.383642   3.587969   2.915865   4.347979   4.637251   \n",
       "17   0.csv_Day7_8h30m   3.891693   6.636632   5.512778   7.801158   8.834262   \n",
       "18   0.csv_Day7_9h00m   5.920645  10.082118   8.638920  11.743272  13.582507   \n",
       "19   0.csv_Day7_9h30m   7.578732  12.495900  13.091483  15.564502  18.910107   \n",
       "20  0.csv_Day7_10h00m   9.202325  14.370083  17.380587  18.922745  21.778227   \n",
       "21  0.csv_Day7_10h30m  10.377932  16.560617  21.049261  22.855680  26.007191   \n",
       "22  0.csv_Day7_11h00m  11.315662  17.851332  23.594387  26.603224  29.110861   \n",
       "23  0.csv_Day7_11h30m  11.923900  18.096512  25.127829  28.512575  31.546825   \n",
       "24  0.csv_Day7_12h00m  12.259184  17.972837  24.602896  29.206108  32.086597   \n",
       "25  0.csv_Day7_12h30m  11.664116  16.960072  23.584049  27.749851  31.522541   \n",
       "26  0.csv_Day7_13h00m  10.191174  16.064186  22.387798  25.843534  29.914156   \n",
       "27  0.csv_Day7_13h30m   8.338793  14.031995  20.028107  23.580259  26.111427   \n",
       "28  0.csv_Day7_14h00m   6.438894  11.798625  17.772745  20.204332  22.745476   \n",
       "29  0.csv_Day7_14h30m   4.642674   8.917039  14.868993  16.636839  19.511023   \n",
       "30  0.csv_Day7_15h00m   2.930609   6.400648  11.751292  12.728077  15.909344   \n",
       "31  0.csv_Day7_15h30m   1.649050   3.933033   8.617319   8.952081  11.080450   \n",
       "32  0.csv_Day7_16h00m   0.974194   1.807476   5.380528   5.751849   5.436456   \n",
       "33  0.csv_Day7_16h30m   0.201524   0.623902   3.130742   2.934508   1.469816   \n",
       "34  0.csv_Day7_17h00m  -0.015940  -0.053902   1.742528   0.971621   0.092404   \n",
       "35  0.csv_Day7_17h30m  -0.043710  -0.165341   1.029288   0.069415  -0.352298   \n",
       "36  0.csv_Day7_18h00m  -0.040556  -0.103839   0.429723  -0.155786  -0.193194   \n",
       "37  0.csv_Day7_18h30m  -0.057124  -0.022649   0.096232  -0.367589  -0.001791   \n",
       "38  0.csv_Day7_19h00m  -0.036151  -0.031596  -0.154644  -0.245674  -0.094616   \n",
       "39  0.csv_Day7_19h30m  -0.024918   0.039934  -0.078590  -0.166324  -0.043196   \n",
       "40  0.csv_Day7_20h00m  -0.039703  -0.014718   0.026451  -0.170821  -0.066030   \n",
       "41  0.csv_Day7_20h30m  -0.040474   0.047775  -0.007638  -0.075785  -0.143889   \n",
       "42  0.csv_Day7_21h00m  -0.044599   0.031964   0.016413  -0.013890   0.031991   \n",
       "43  0.csv_Day7_21h30m  -0.040397   0.017927  -0.026830  -0.018662  -0.084809   \n",
       "44  0.csv_Day7_22h00m  -0.021974   0.001427  -0.078171  -0.085914  -0.109154   \n",
       "45  0.csv_Day7_22h30m   0.002654  -0.031366  -0.034161  -0.092709  -0.065930   \n",
       "46  0.csv_Day7_23h00m   0.015975  -0.090818  -0.108225  -0.112969  -0.039415   \n",
       "47  0.csv_Day7_23h30m  -0.010059  -0.039430  -0.084664  -0.129694  -0.097498   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0   -0.022728  -0.108168   0.074562   0.595425  \n",
       "1    0.082372  -0.084517  -0.059915   1.063461  \n",
       "2    0.032507   0.035325   0.019248   1.504606  \n",
       "3    0.199537   0.171587   0.039975   1.511725  \n",
       "4    0.453990   0.407158   0.145288   1.324745  \n",
       "5    0.295865   0.312074   0.099051   0.968625  \n",
       "6    0.151301   0.406871   0.055930   0.957362  \n",
       "7   -0.038526   0.247233   0.030798   0.219046  \n",
       "8   -0.103541   0.319506   0.039582   0.495112  \n",
       "9    0.042954   0.484153   0.431663   0.559823  \n",
       "10   0.295156   0.441591   0.528161  -0.087109  \n",
       "11   0.324341   0.190489   0.708684   0.617961  \n",
       "12   0.096941   0.089325   1.246120   1.621673  \n",
       "13  -0.053907   0.441158   1.455138   3.089519  \n",
       "14   0.591490   1.744735   2.592998   5.093650  \n",
       "15   1.915306   4.540763   5.262686   8.937643  \n",
       "16   4.823846   8.629528   9.494079  13.571690  \n",
       "17   9.745342  14.793843  15.650631  19.707041  \n",
       "18  15.869155  20.750513  22.549725  26.517036  \n",
       "19  21.792852  27.005182  29.240368  34.093887  \n",
       "20  26.887941  33.066570  35.087997  40.060902  \n",
       "21  30.495989  37.841412  39.055458  44.992802  \n",
       "22  32.556499  41.089462  41.923168  48.452892  \n",
       "23  35.788204  43.759438  43.584980  49.854446  \n",
       "24  37.434673  44.159756  44.015755  50.155651  \n",
       "25  36.887539  44.263329  43.224564  48.869244  \n",
       "26  35.671467  42.116119  40.326588  46.095654  \n",
       "27  33.341022  38.915386  36.632172  42.542233  \n",
       "28  30.142757  34.384727  32.552292  37.896477  \n",
       "29  25.442408  28.204422  27.932016  32.630417  \n",
       "30  19.699924  20.741110  21.636147  25.989923  \n",
       "31  13.816643  14.024981  15.228221  19.284466  \n",
       "32   7.641447   8.822656   9.219543  12.351213  \n",
       "33   3.469676   5.192949   5.178250   7.116824  \n",
       "34   0.868362   2.794237   3.109246   3.915729  \n",
       "35  -0.082172   1.894108   2.155954   2.454201  \n",
       "36  -0.187773   1.143442   1.366849   1.423467  \n",
       "37  -0.059490   1.029982   0.804624   0.757860  \n",
       "38  -0.124035   0.505330   0.283033   0.461291  \n",
       "39  -0.143338   0.338303  -0.070067   0.242706  \n",
       "40  -0.101935   0.066204   0.110954   0.373869  \n",
       "41  -0.159693  -0.094389  -0.194270   0.490138  \n",
       "42  -0.052794   0.114666  -0.047797   0.169868  \n",
       "43  -0.035200   0.142241  -0.186230   0.260162  \n",
       "44  -0.090836  -0.059531  -0.018495   0.028261  \n",
       "45  -0.013743   0.116565   0.130139  -0.095092  \n",
       "46  -0.071323   0.076300   0.177514  -0.042162  \n",
       "47  -0.092663   0.146892   0.271685   0.165213  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df[0:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"2D_cnn_mean_shift_zero.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074562</td>\n",
       "      <td>0.595425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023997</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.082372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032507</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>1.504606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076161</td>\n",
       "      <td>0.061755</td>\n",
       "      <td>0.199537</td>\n",
       "      <td>0.171587</td>\n",
       "      <td>0.039975</td>\n",
       "      <td>1.511725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>0.453990</td>\n",
       "      <td>0.407158</td>\n",
       "      <td>0.145288</td>\n",
       "      <td>1.324745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.295865</td>\n",
       "      <td>0.312074</td>\n",
       "      <td>0.099051</td>\n",
       "      <td>0.968625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.151301</td>\n",
       "      <td>0.406871</td>\n",
       "      <td>0.055930</td>\n",
       "      <td>0.957362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247233</td>\n",
       "      <td>0.030798</td>\n",
       "      <td>0.219046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036708</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319506</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.495112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134667</td>\n",
       "      <td>0.109588</td>\n",
       "      <td>0.042954</td>\n",
       "      <td>0.484153</td>\n",
       "      <td>0.431663</td>\n",
       "      <td>0.559823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258881</td>\n",
       "      <td>0.202406</td>\n",
       "      <td>0.295156</td>\n",
       "      <td>0.441591</td>\n",
       "      <td>0.528161</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095863</td>\n",
       "      <td>0.194881</td>\n",
       "      <td>0.324341</td>\n",
       "      <td>0.190489</td>\n",
       "      <td>0.708684</td>\n",
       "      <td>0.617961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049515</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.089325</td>\n",
       "      <td>1.246120</td>\n",
       "      <td>1.621673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441158</td>\n",
       "      <td>1.455138</td>\n",
       "      <td>3.089519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.030976</td>\n",
       "      <td>0.388660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591490</td>\n",
       "      <td>1.744735</td>\n",
       "      <td>2.592998</td>\n",
       "      <td>5.093650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>1.145264</td>\n",
       "      <td>1.771394</td>\n",
       "      <td>0.622307</td>\n",
       "      <td>1.797454</td>\n",
       "      <td>1.238429</td>\n",
       "      <td>1.915306</td>\n",
       "      <td>4.540763</td>\n",
       "      <td>5.262686</td>\n",
       "      <td>8.937643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>2.383642</td>\n",
       "      <td>3.587969</td>\n",
       "      <td>2.915865</td>\n",
       "      <td>4.347979</td>\n",
       "      <td>4.637251</td>\n",
       "      <td>4.823846</td>\n",
       "      <td>8.629528</td>\n",
       "      <td>9.494079</td>\n",
       "      <td>13.571690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>3.891693</td>\n",
       "      <td>6.636632</td>\n",
       "      <td>5.512778</td>\n",
       "      <td>7.801158</td>\n",
       "      <td>8.834262</td>\n",
       "      <td>9.745342</td>\n",
       "      <td>14.793843</td>\n",
       "      <td>15.650631</td>\n",
       "      <td>19.707041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.920645</td>\n",
       "      <td>10.082118</td>\n",
       "      <td>8.638920</td>\n",
       "      <td>11.743272</td>\n",
       "      <td>13.582507</td>\n",
       "      <td>15.869155</td>\n",
       "      <td>20.750513</td>\n",
       "      <td>22.549725</td>\n",
       "      <td>26.517036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>7.578732</td>\n",
       "      <td>12.495900</td>\n",
       "      <td>13.091483</td>\n",
       "      <td>15.564502</td>\n",
       "      <td>18.910107</td>\n",
       "      <td>21.792852</td>\n",
       "      <td>27.005182</td>\n",
       "      <td>29.240368</td>\n",
       "      <td>34.093887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>9.202325</td>\n",
       "      <td>14.370083</td>\n",
       "      <td>17.380587</td>\n",
       "      <td>18.922745</td>\n",
       "      <td>21.778227</td>\n",
       "      <td>26.887941</td>\n",
       "      <td>33.066570</td>\n",
       "      <td>35.087997</td>\n",
       "      <td>40.060902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>10.377932</td>\n",
       "      <td>16.560617</td>\n",
       "      <td>21.049261</td>\n",
       "      <td>22.855680</td>\n",
       "      <td>26.007191</td>\n",
       "      <td>30.495989</td>\n",
       "      <td>37.841412</td>\n",
       "      <td>39.055458</td>\n",
       "      <td>44.992802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>11.315662</td>\n",
       "      <td>17.851332</td>\n",
       "      <td>23.594387</td>\n",
       "      <td>26.603224</td>\n",
       "      <td>29.110861</td>\n",
       "      <td>32.556499</td>\n",
       "      <td>41.089462</td>\n",
       "      <td>41.923168</td>\n",
       "      <td>48.452892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>11.923900</td>\n",
       "      <td>18.096512</td>\n",
       "      <td>25.127829</td>\n",
       "      <td>28.512575</td>\n",
       "      <td>31.546825</td>\n",
       "      <td>35.788204</td>\n",
       "      <td>43.759438</td>\n",
       "      <td>43.584980</td>\n",
       "      <td>49.854446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>12.259184</td>\n",
       "      <td>17.972837</td>\n",
       "      <td>24.602896</td>\n",
       "      <td>29.206108</td>\n",
       "      <td>32.086597</td>\n",
       "      <td>37.434673</td>\n",
       "      <td>44.159756</td>\n",
       "      <td>44.015755</td>\n",
       "      <td>50.155651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>11.664116</td>\n",
       "      <td>16.960072</td>\n",
       "      <td>23.584049</td>\n",
       "      <td>27.749851</td>\n",
       "      <td>31.522541</td>\n",
       "      <td>36.887539</td>\n",
       "      <td>44.263329</td>\n",
       "      <td>43.224564</td>\n",
       "      <td>48.869244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>10.191174</td>\n",
       "      <td>16.064186</td>\n",
       "      <td>22.387798</td>\n",
       "      <td>25.843534</td>\n",
       "      <td>29.914156</td>\n",
       "      <td>35.671467</td>\n",
       "      <td>42.116119</td>\n",
       "      <td>40.326588</td>\n",
       "      <td>46.095654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>8.338793</td>\n",
       "      <td>14.031995</td>\n",
       "      <td>20.028107</td>\n",
       "      <td>23.580259</td>\n",
       "      <td>26.111427</td>\n",
       "      <td>33.341022</td>\n",
       "      <td>38.915386</td>\n",
       "      <td>36.632172</td>\n",
       "      <td>42.542233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>6.438894</td>\n",
       "      <td>11.798625</td>\n",
       "      <td>17.772745</td>\n",
       "      <td>20.204332</td>\n",
       "      <td>22.745476</td>\n",
       "      <td>30.142757</td>\n",
       "      <td>34.384727</td>\n",
       "      <td>32.552292</td>\n",
       "      <td>37.896477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>4.642674</td>\n",
       "      <td>8.917039</td>\n",
       "      <td>14.868993</td>\n",
       "      <td>16.636839</td>\n",
       "      <td>19.511023</td>\n",
       "      <td>25.442408</td>\n",
       "      <td>28.204422</td>\n",
       "      <td>27.932016</td>\n",
       "      <td>32.630417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>2.930609</td>\n",
       "      <td>6.400648</td>\n",
       "      <td>11.751292</td>\n",
       "      <td>12.728077</td>\n",
       "      <td>15.909344</td>\n",
       "      <td>19.699924</td>\n",
       "      <td>20.741110</td>\n",
       "      <td>21.636147</td>\n",
       "      <td>25.989923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>1.649050</td>\n",
       "      <td>3.933033</td>\n",
       "      <td>8.617319</td>\n",
       "      <td>8.952081</td>\n",
       "      <td>11.080450</td>\n",
       "      <td>13.816643</td>\n",
       "      <td>14.024981</td>\n",
       "      <td>15.228221</td>\n",
       "      <td>19.284466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>1.807476</td>\n",
       "      <td>5.380528</td>\n",
       "      <td>5.751849</td>\n",
       "      <td>5.436456</td>\n",
       "      <td>7.641447</td>\n",
       "      <td>8.822656</td>\n",
       "      <td>9.219543</td>\n",
       "      <td>12.351213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>0.201524</td>\n",
       "      <td>0.623902</td>\n",
       "      <td>3.130742</td>\n",
       "      <td>2.934508</td>\n",
       "      <td>1.469816</td>\n",
       "      <td>3.469676</td>\n",
       "      <td>5.192949</td>\n",
       "      <td>5.178250</td>\n",
       "      <td>7.116824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.742528</td>\n",
       "      <td>0.971621</td>\n",
       "      <td>0.092404</td>\n",
       "      <td>0.868362</td>\n",
       "      <td>2.794237</td>\n",
       "      <td>3.109246</td>\n",
       "      <td>3.915729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.029288</td>\n",
       "      <td>0.069415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.894108</td>\n",
       "      <td>2.155954</td>\n",
       "      <td>2.454201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.143442</td>\n",
       "      <td>1.366849</td>\n",
       "      <td>1.423467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.029982</td>\n",
       "      <td>0.804624</td>\n",
       "      <td>0.757860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505330</td>\n",
       "      <td>0.283033</td>\n",
       "      <td>0.461291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066204</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.373869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031964</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116565</td>\n",
       "      <td>0.130139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.177514</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146892</td>\n",
       "      <td>0.271685</td>\n",
       "      <td>0.165213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m   0.000000   0.000000   0.000000   0.063773   0.000000   \n",
       "1    0.csv_Day7_0h30m   0.000000   0.013037   0.000000   0.023997   0.006828   \n",
       "2    0.csv_Day7_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.csv_Day7_1h30m   0.000000   0.000000   0.000000   0.076161   0.061755   \n",
       "4    0.csv_Day7_2h00m   0.000000   0.000744   0.000000   0.000000   0.035461   \n",
       "5    0.csv_Day7_2h30m   0.000000   0.000839   0.000000   0.000000   0.020227   \n",
       "6    0.csv_Day7_3h00m   0.002505   0.028633   0.000000   0.000000   0.148333   \n",
       "7    0.csv_Day7_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.csv_Day7_4h00m   0.000000   0.000000   0.000000   0.036708   0.017185   \n",
       "9    0.csv_Day7_4h30m   0.000000   0.000000   0.000000   0.134667   0.109588   \n",
       "10   0.csv_Day7_5h00m   0.000000   0.000000   0.000000   0.258881   0.202406   \n",
       "11   0.csv_Day7_5h30m   0.000000   0.000000   0.000000   0.095863   0.194881   \n",
       "12   0.csv_Day7_6h00m   0.000000   0.000000   0.000000   0.000000   0.049515   \n",
       "13   0.csv_Day7_6h30m   0.000000   0.032490   0.000000   0.000000   0.000000   \n",
       "14   0.csv_Day7_7h00m   0.030976   0.388660   0.000000   0.231003   0.000000   \n",
       "15   0.csv_Day7_7h30m   1.145264   1.771394   0.622307   1.797454   1.238429   \n",
       "16   0.csv_Day7_8h00m   2.383642   3.587969   2.915865   4.347979   4.637251   \n",
       "17   0.csv_Day7_8h30m   3.891693   6.636632   5.512778   7.801158   8.834262   \n",
       "18   0.csv_Day7_9h00m   5.920645  10.082118   8.638920  11.743272  13.582507   \n",
       "19   0.csv_Day7_9h30m   7.578732  12.495900  13.091483  15.564502  18.910107   \n",
       "20  0.csv_Day7_10h00m   9.202325  14.370083  17.380587  18.922745  21.778227   \n",
       "21  0.csv_Day7_10h30m  10.377932  16.560617  21.049261  22.855680  26.007191   \n",
       "22  0.csv_Day7_11h00m  11.315662  17.851332  23.594387  26.603224  29.110861   \n",
       "23  0.csv_Day7_11h30m  11.923900  18.096512  25.127829  28.512575  31.546825   \n",
       "24  0.csv_Day7_12h00m  12.259184  17.972837  24.602896  29.206108  32.086597   \n",
       "25  0.csv_Day7_12h30m  11.664116  16.960072  23.584049  27.749851  31.522541   \n",
       "26  0.csv_Day7_13h00m  10.191174  16.064186  22.387798  25.843534  29.914156   \n",
       "27  0.csv_Day7_13h30m   8.338793  14.031995  20.028107  23.580259  26.111427   \n",
       "28  0.csv_Day7_14h00m   6.438894  11.798625  17.772745  20.204332  22.745476   \n",
       "29  0.csv_Day7_14h30m   4.642674   8.917039  14.868993  16.636839  19.511023   \n",
       "30  0.csv_Day7_15h00m   2.930609   6.400648  11.751292  12.728077  15.909344   \n",
       "31  0.csv_Day7_15h30m   1.649050   3.933033   8.617319   8.952081  11.080450   \n",
       "32  0.csv_Day7_16h00m   0.974194   1.807476   5.380528   5.751849   5.436456   \n",
       "33  0.csv_Day7_16h30m   0.201524   0.623902   3.130742   2.934508   1.469816   \n",
       "34  0.csv_Day7_17h00m   0.000000   0.000000   1.742528   0.971621   0.092404   \n",
       "35  0.csv_Day7_17h30m   0.000000   0.000000   1.029288   0.069415   0.000000   \n",
       "36  0.csv_Day7_18h00m   0.000000   0.000000   0.429723   0.000000   0.000000   \n",
       "37  0.csv_Day7_18h30m   0.000000   0.000000   0.096232   0.000000   0.000000   \n",
       "38  0.csv_Day7_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  0.csv_Day7_19h30m   0.000000   0.039934   0.000000   0.000000   0.000000   \n",
       "40  0.csv_Day7_20h00m   0.000000   0.000000   0.026451   0.000000   0.000000   \n",
       "41  0.csv_Day7_20h30m   0.000000   0.047775   0.000000   0.000000   0.000000   \n",
       "42  0.csv_Day7_21h00m   0.000000   0.031964   0.016413   0.000000   0.031991   \n",
       "43  0.csv_Day7_21h30m   0.000000   0.017927   0.000000   0.000000   0.000000   \n",
       "44  0.csv_Day7_22h00m   0.000000   0.001427   0.000000   0.000000   0.000000   \n",
       "45  0.csv_Day7_22h30m   0.002654   0.000000   0.000000   0.000000   0.000000   \n",
       "46  0.csv_Day7_23h00m   0.015975   0.000000   0.000000   0.000000   0.000000   \n",
       "47  0.csv_Day7_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.000000   0.074562   0.595425  \n",
       "1    0.082372   0.000000   0.000000   1.063461  \n",
       "2    0.032507   0.035325   0.019248   1.504606  \n",
       "3    0.199537   0.171587   0.039975   1.511725  \n",
       "4    0.453990   0.407158   0.145288   1.324745  \n",
       "5    0.295865   0.312074   0.099051   0.968625  \n",
       "6    0.151301   0.406871   0.055930   0.957362  \n",
       "7    0.000000   0.247233   0.030798   0.219046  \n",
       "8    0.000000   0.319506   0.039582   0.495112  \n",
       "9    0.042954   0.484153   0.431663   0.559823  \n",
       "10   0.295156   0.441591   0.528161   0.000000  \n",
       "11   0.324341   0.190489   0.708684   0.617961  \n",
       "12   0.096941   0.089325   1.246120   1.621673  \n",
       "13   0.000000   0.441158   1.455138   3.089519  \n",
       "14   0.591490   1.744735   2.592998   5.093650  \n",
       "15   1.915306   4.540763   5.262686   8.937643  \n",
       "16   4.823846   8.629528   9.494079  13.571690  \n",
       "17   9.745342  14.793843  15.650631  19.707041  \n",
       "18  15.869155  20.750513  22.549725  26.517036  \n",
       "19  21.792852  27.005182  29.240368  34.093887  \n",
       "20  26.887941  33.066570  35.087997  40.060902  \n",
       "21  30.495989  37.841412  39.055458  44.992802  \n",
       "22  32.556499  41.089462  41.923168  48.452892  \n",
       "23  35.788204  43.759438  43.584980  49.854446  \n",
       "24  37.434673  44.159756  44.015755  50.155651  \n",
       "25  36.887539  44.263329  43.224564  48.869244  \n",
       "26  35.671467  42.116119  40.326588  46.095654  \n",
       "27  33.341022  38.915386  36.632172  42.542233  \n",
       "28  30.142757  34.384727  32.552292  37.896477  \n",
       "29  25.442408  28.204422  27.932016  32.630417  \n",
       "30  19.699924  20.741110  21.636147  25.989923  \n",
       "31  13.816643  14.024981  15.228221  19.284466  \n",
       "32   7.641447   8.822656   9.219543  12.351213  \n",
       "33   3.469676   5.192949   5.178250   7.116824  \n",
       "34   0.868362   2.794237   3.109246   3.915729  \n",
       "35   0.000000   1.894108   2.155954   2.454201  \n",
       "36   0.000000   1.143442   1.366849   1.423467  \n",
       "37   0.000000   1.029982   0.804624   0.757860  \n",
       "38   0.000000   0.505330   0.283033   0.461291  \n",
       "39   0.000000   0.338303   0.000000   0.242706  \n",
       "40   0.000000   0.066204   0.110954   0.373869  \n",
       "41   0.000000   0.000000   0.000000   0.490138  \n",
       "42   0.000000   0.114666   0.000000   0.169868  \n",
       "43   0.000000   0.142241   0.000000   0.260162  \n",
       "44   0.000000   0.000000   0.000000   0.028261  \n",
       "45   0.000000   0.116565   0.130139   0.000000  \n",
       "46   0.000000   0.076300   0.177514   0.000000  \n",
       "47   0.000000   0.146892   0.271685   0.165213  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    q = i/10\n",
    "    submission_df[f\"q_{q}\"][submission_df[f\"q_{q}\"] < 0] = 0\n",
    "submission_df[0:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.3: [1.505661964416504,\n",
       "  3.5561978816986084,\n",
       "  56.22322082519531,\n",
       "  1.5056190490722656]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{0.3: [1.59465754032135,\n",
    "  3.8922789096832275,\n",
    "  62.40233612060547,\n",
    "  1.5945602655410767]}\n",
    "{0.3: [1.5206161737442017,\n",
    "  3.520108461380005,\n",
    "  55.35062026977539,\n",
    "  1.5206190347671509]}\n",
    "{0.3: [1.505661964416504,\n",
    "  3.5561978816986084,\n",
    "  56.22322082519531,\n",
    "  1.5056190490722656]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bees1\\anaconda3\\envs\\dacon\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.075737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.665925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051865</td>\n",
       "      <td>0.429422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125713</td>\n",
       "      <td>0.132835</td>\n",
       "      <td>0.425520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341038</td>\n",
       "      <td>0.397704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165248</td>\n",
       "      <td>0.049708</td>\n",
       "      <td>0.367383</td>\n",
       "      <td>0.262605</td>\n",
       "      <td>0.636854</td>\n",
       "      <td>0.203558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.617736</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>0.385116</td>\n",
       "      <td>0.566612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.013990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323615</td>\n",
       "      <td>0.231200</td>\n",
       "      <td>0.554969</td>\n",
       "      <td>0.536913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123434</td>\n",
       "      <td>0.237913</td>\n",
       "      <td>0.787921</td>\n",
       "      <td>0.529678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692273</td>\n",
       "      <td>0.418214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085588</td>\n",
       "      <td>0.066988</td>\n",
       "      <td>1.056911</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.004436</td>\n",
       "      <td>0.149318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960197</td>\n",
       "      <td>0.274340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187459</td>\n",
       "      <td>1.003103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843586</td>\n",
       "      <td>0.065198</td>\n",
       "      <td>2.301388</td>\n",
       "      <td>2.199021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492276</td>\n",
       "      <td>2.665259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316023</td>\n",
       "      <td>2.723710</td>\n",
       "      <td>1.775952</td>\n",
       "      <td>3.998145</td>\n",
       "      <td>5.423578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.589180</td>\n",
       "      <td>4.855118</td>\n",
       "      <td>0.778672</td>\n",
       "      <td>2.880782</td>\n",
       "      <td>5.520342</td>\n",
       "      <td>6.410540</td>\n",
       "      <td>7.064509</td>\n",
       "      <td>11.412713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.761744</td>\n",
       "      <td>7.857065</td>\n",
       "      <td>4.049734</td>\n",
       "      <td>6.125292</td>\n",
       "      <td>9.419258</td>\n",
       "      <td>12.569093</td>\n",
       "      <td>13.103045</td>\n",
       "      <td>17.501560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.051432</td>\n",
       "      <td>11.461951</td>\n",
       "      <td>8.429073</td>\n",
       "      <td>10.919353</td>\n",
       "      <td>14.156559</td>\n",
       "      <td>19.976631</td>\n",
       "      <td>20.243694</td>\n",
       "      <td>23.619379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>9.290476</td>\n",
       "      <td>15.885969</td>\n",
       "      <td>13.368224</td>\n",
       "      <td>15.005770</td>\n",
       "      <td>19.588079</td>\n",
       "      <td>26.758629</td>\n",
       "      <td>27.184805</td>\n",
       "      <td>29.851683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     q_0.1     q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0   0.csv_Day7_0h00m  0.000000  0.000000   0.000000   0.000000   0.018542   \n",
       "1   0.csv_Day7_0h30m  0.000000  0.022888   0.000000   0.000000   0.000000   \n",
       "2   0.csv_Day7_1h00m  0.000000  0.000000   0.006969   0.075737   0.000000   \n",
       "3   0.csv_Day7_1h30m  0.000000  0.002482   0.000000   0.149559   0.000000   \n",
       "4   0.csv_Day7_2h00m  0.000000  0.004925   0.000000   0.125713   0.132835   \n",
       "5   0.csv_Day7_2h30m  0.000000  0.005199   0.000000   0.165248   0.049708   \n",
       "6   0.csv_Day7_3h00m  0.000056  0.000000   0.000000   0.000000   0.035667   \n",
       "7   0.csv_Day7_3h30m  0.000058  0.013990   0.000000   0.000000   0.000000   \n",
       "8   0.csv_Day7_4h00m  0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "9   0.csv_Day7_4h30m  0.000128  0.000000   0.000000   0.000000   0.000000   \n",
       "10  0.csv_Day7_5h00m  0.000000  0.009108   0.000000   0.000000   0.000000   \n",
       "11  0.csv_Day7_5h30m  0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "12  0.csv_Day7_6h00m  0.000000  0.012701   0.000000   0.000000   0.000000   \n",
       "13  0.csv_Day7_6h30m  0.000043  0.000000   0.292543   0.000000   0.000000   \n",
       "14  0.csv_Day7_7h00m  0.000161  0.000000   1.142528   0.000000   0.000000   \n",
       "15  0.csv_Day7_7h30m  0.000000  0.492276   2.665259   0.000000   0.316023   \n",
       "16  0.csv_Day7_8h00m  0.000076  1.589180   4.855118   0.778672   2.880782   \n",
       "17  0.csv_Day7_8h30m  0.000000  3.761744   7.857065   4.049734   6.125292   \n",
       "18  0.csv_Day7_9h00m  0.000000  6.051432  11.461951   8.429073  10.919353   \n",
       "19  0.csv_Day7_9h30m  0.000333  9.290476  15.885969  13.368224  15.005770   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.006062   0.000000   0.131488  \n",
       "1    0.000000   0.000000   0.000000   0.073663  \n",
       "2    0.000000   0.000000   0.000000   0.665925  \n",
       "3    0.174456   0.000000   0.051865   0.429422  \n",
       "4    0.425520   0.000000   0.341038   0.397704  \n",
       "5    0.367383   0.262605   0.636854   0.203558  \n",
       "6    0.617736   0.207500   0.385116   0.566612  \n",
       "7    0.323615   0.231200   0.554969   0.536913  \n",
       "8    0.123434   0.237913   0.787921   0.529678  \n",
       "9    0.036401   0.000000   0.692273   0.418214  \n",
       "10   0.085588   0.066988   1.056911   0.000000  \n",
       "11   0.086340   0.000000   1.004436   0.149318  \n",
       "12   0.038318   0.000000   0.960197   0.274340  \n",
       "13   0.145326   0.000000   1.187459   1.003103  \n",
       "14   0.843586   0.065198   2.301388   2.199021  \n",
       "15   2.723710   1.775952   3.998145   5.423578  \n",
       "16   5.520342   6.410540   7.064509  11.412713  \n",
       "17   9.419258  12.569093  13.103045  17.501560  \n",
       "18  14.156559  19.976631  20.243694  23.619379  \n",
       "19  19.588079  26.758629  27.184805  29.851683  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    q = i/10\n",
    "    submission_df[f\"q_{q}\"][submission_df[f\"q_{q}\"] < 0] = 0\n",
    "\n",
    "submission_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{0.1: [0.8983700275421143,\n",
    "  6.469372272491455,\n",
    "  157.601806640625,\n",
    "  0.898374617099762]}  \n",
    "  {0.1: [0.8474237322807312,\n",
    "  6.678616046905518,\n",
    "  168.7744140625,\n",
    "  0.8474520444869995]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio should be 3 length list, example: [0.6, 0.3, 0.1]\n",
      "sum of ration should be 1, example: [0.7, 0.2, 0.1]\n",
      "shape of train, valid, test: (36792, 9), (10511, 9), (5257, 9)\n",
      "ratio should be 3 length list, example: [0.6, 0.3, 0.1]\n",
      "sum of ration should be 1, example: [0.7, 0.2, 0.1]\n",
      "shape of train, valid, test: (36792, 16), (10511, 16), (5257, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>Day_sin</th>\n",
       "      <th>Day_cos</th>\n",
       "      <th>Hour_sin</th>\n",
       "      <th>Hour_cos</th>\n",
       "      <th>GHI</th>\n",
       "      <th>TARGET_ROLLING_MEAN_3_shift_1</th>\n",
       "      <th>TARGET_ROLLING_MEAN_5_shift_2</th>\n",
       "      <th>TARGET_ROLLING_MEAN_11_shift_5</th>\n",
       "      <th>TARGET_ROLLING_MEAN_23_shift_11</th>\n",
       "      <th>TARGET_ROLLING_MEAN_47_shift_23</th>\n",
       "      <th>scaled_TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>3.679200e+04</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>3.679200e+04</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "      <td>36792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.542351</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>17.439978</td>\n",
       "      <td>0.507046</td>\n",
       "      <td>0.522326</td>\n",
       "      <td>0.500206</td>\n",
       "      <td>0.500027</td>\n",
       "      <td>0.267026</td>\n",
       "      <td>1.754282e-01</td>\n",
       "      <td>0.177078</td>\n",
       "      <td>1.873781e-01</td>\n",
       "      <td>0.239824</td>\n",
       "      <td>0.469365</td>\n",
       "      <td>0.174550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.194232</td>\n",
       "      <td>0.330292</td>\n",
       "      <td>0.117052</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>0.192822</td>\n",
       "      <td>25.449577</td>\n",
       "      <td>0.346981</td>\n",
       "      <td>0.359263</td>\n",
       "      <td>0.353558</td>\n",
       "      <td>0.353558</td>\n",
       "      <td>0.263381</td>\n",
       "      <td>2.493108e-01</td>\n",
       "      <td>0.245816</td>\n",
       "      <td>2.357651e-01</td>\n",
       "      <td>0.221283</td>\n",
       "      <td>0.211497</td>\n",
       "      <td>0.254715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.364896</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159610</td>\n",
       "      <td>0.158025</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.088432</td>\n",
       "      <td>3.356260e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.852145e-16</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>0.297058</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530107</td>\n",
       "      <td>0.536539</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.088432</td>\n",
       "      <td>1.006823e-02</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>7.214131e-02</td>\n",
       "      <td>0.186432</td>\n",
       "      <td>0.447393</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.160985</td>\n",
       "      <td>0.430831</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.711963</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>31.085053</td>\n",
       "      <td>0.840390</td>\n",
       "      <td>0.891488</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>3.152806e-01</td>\n",
       "      <td>0.319378</td>\n",
       "      <td>3.256305e-01</td>\n",
       "      <td>0.378664</td>\n",
       "      <td>0.634329</td>\n",
       "      <td>0.311118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.913939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DHI           DNI            WS            RH             T  \\\n",
       "count  36792.000000  36792.000000  36792.000000  36792.000000  36792.000000   \n",
       "mean       0.119412      0.219731      0.201327      0.542351      0.512100   \n",
       "std        0.194232      0.330292      0.117052      0.234989      0.192822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.116667      0.364896      0.351852   \n",
       "50%        0.000000      0.000000      0.183333      0.553620      0.500000   \n",
       "75%        0.160985      0.430831      0.258333      0.711963      0.648148   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             TARGET       Day_sin       Day_cos      Hour_sin      Hour_cos  \\\n",
       "count  36792.000000  36792.000000  36792.000000  36792.000000  36792.000000   \n",
       "mean      17.439978      0.507046      0.522326      0.500206      0.500027   \n",
       "std       25.449577      0.346981      0.359263      0.353558      0.353558   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.159610      0.158025      0.146447      0.146447   \n",
       "50%        0.000000      0.530107      0.536539      0.500000      0.500000   \n",
       "75%       31.085053      0.840390      0.891488      0.853553      0.853553   \n",
       "max       99.913939      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                GHI  TARGET_ROLLING_MEAN_3_shift_1  \\\n",
       "count  36792.000000                   3.679200e+04   \n",
       "mean       0.267026                   1.754282e-01   \n",
       "std        0.263381                   2.493108e-01   \n",
       "min        0.000000                   0.000000e+00   \n",
       "25%        0.088432                   3.356260e-15   \n",
       "50%        0.088432                   1.006823e-02   \n",
       "75%        0.415900                   3.152806e-01   \n",
       "max        1.000000                   1.000000e+00   \n",
       "\n",
       "       TARGET_ROLLING_MEAN_5_shift_2  TARGET_ROLLING_MEAN_11_shift_5  \\\n",
       "count                   36792.000000                    3.679200e+04   \n",
       "mean                        0.177078                    1.873781e-01   \n",
       "std                         0.245816                    2.357651e-01   \n",
       "min                         0.000000                    0.000000e+00   \n",
       "25%                         0.000000                    1.852145e-16   \n",
       "50%                         0.023250                    7.214131e-02   \n",
       "75%                         0.319378                    3.256305e-01   \n",
       "max                         1.000000                    1.000000e+00   \n",
       "\n",
       "       TARGET_ROLLING_MEAN_23_shift_11  TARGET_ROLLING_MEAN_47_shift_23  \\\n",
       "count                     36792.000000                     36792.000000   \n",
       "mean                          0.239824                         0.469365   \n",
       "std                           0.221283                         0.211497   \n",
       "min                           0.000000                         0.000000   \n",
       "25%                           0.044158                         0.297058   \n",
       "50%                           0.186432                         0.447393   \n",
       "75%                           0.378664                         0.634329   \n",
       "max                           1.000000                         1.000000   \n",
       "\n",
       "       scaled_TARGET  \n",
       "count   36792.000000  \n",
       "mean        0.174550  \n",
       "std         0.254715  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.311118  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df, test_df = split_train_valid_test(df, [0.7, 0.2, 0.1])\n",
    "\n",
    "train_df_target = train_df[\"TARGET\"]\n",
    "valid_df_target = valid_df[\"TARGET\"]\n",
    "test_df_target = test_df[\"TARGET\"]\n",
    "\n",
    "train_df, valid_df, test_df = load_basic_preprocessed_train(\"minmax\")\n",
    "\n",
    "train_df[\"scaled_TARGET\"] = train_df[\"TARGET\"]\n",
    "valid_df[\"scaled_TARGET\"] = valid_df[\"TARGET\"]\n",
    "test_df[\"scaled_TARGET\"] = test_df[\"TARGET\"]\n",
    "\n",
    "train_df[\"TARGET\"] = train_df_target\n",
    "valid_df[\"TARGET\"] = valid_df_target\n",
    "test_df[\"TARGET\"] = test_df_target\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = [\n",
    "    'DHI', \n",
    "    'DNI', \n",
    "     'WS', \n",
    "    'RH', \n",
    "    'T', \n",
    "     'Hour_sin',\n",
    "     'Hour_cos', \n",
    "    'GHI', \n",
    "     'TARGET_ROLLING_MEAN_3_shift_1',\n",
    "     'TARGET_ROLLING_MEAN_5_shift_2', \n",
    "     'TARGET_ROLLING_MEAN_11_shift_5',\n",
    "     'TARGET_ROLLING_MEAN_23_shift_11', \n",
    "     'TARGET_ROLLING_MEAN_47_shift_23',\n",
    "    'scaled_TARGET',\n",
    "    \"TARGET\",\n",
    "]\n",
    "train_df = train_df[cutter]\n",
    "valid_df = valid_df[cutter]\n",
    "test_df = test_df[cutter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio should be 3 length list, example: [0.6, 0.3, 0.1]\n",
      "sum of ration should be 1, example: [0.7, 0.2, 0.1]\n",
      "shape of train, valid, test: (36792, 16), (10511, 16), (5257, 16)\n",
      "0.1\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 26s 16ms/step - loss: 1.5047 - mean_absolute_error: 13.6584 - mean_squared_error: 598.4279 - _pinball_loss: 1.5047 - val_loss: 1.6736 - val_mean_absolute_error: 13.5540 - val_mean_squared_error: 529.3325 - val__pinball_loss: 1.6736\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 1.2991 - mean_absolute_error: 10.7893 - mean_squared_error: 383.3972 - _pinball_loss: 1.2991 - val_loss: 1.6314 - val_mean_absolute_error: 13.0589 - val_mean_squared_error: 508.9373 - val__pinball_loss: 1.6314\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 1.2680 - mean_absolute_error: 10.4794 - mean_squared_error: 370.3380 - _pinball_loss: 1.2680 - val_loss: 1.6104 - val_mean_absolute_error: 13.0681 - val_mean_squared_error: 510.0778 - val__pinball_loss: 1.6104\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.2468 - mean_absolute_error: 10.2531 - mean_squared_error: 360.4882 - _pinball_loss: 1.2468 - val_loss: 1.5981 - val_mean_absolute_error: 12.9394 - val_mean_squared_error: 505.4697 - val__pinball_loss: 1.5981\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.2321 - mean_absolute_error: 10.1290 - mean_squared_error: 355.5213 - _pinball_loss: 1.2321 - val_loss: 1.5804 - val_mean_absolute_error: 12.7439 - val_mean_squared_error: 493.8891 - val__pinball_loss: 1.5804\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 1.2201 - mean_absolute_error: 9.9983 - mean_squared_error: 350.0172 - _pinball_loss: 1.2201 - val_loss: 1.5752 - val_mean_absolute_error: 12.7930 - val_mean_squared_error: 496.1561 - val__pinball_loss: 1.5752\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 1.2124 - mean_absolute_error: 9.9334 - mean_squared_error: 347.6757 - _pinball_loss: 1.2124 - val_loss: 1.5625 - val_mean_absolute_error: 12.5509 - val_mean_squared_error: 489.6274 - val__pinball_loss: 1.5625\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.2048 - mean_absolute_error: 9.8721 - mean_squared_error: 345.6752 - _pinball_loss: 1.2048 - val_loss: 1.5600 - val_mean_absolute_error: 12.6697 - val_mean_squared_error: 496.2101 - val__pinball_loss: 1.5600\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1973 - mean_absolute_error: 9.8275 - mean_squared_error: 343.8624 - _pinball_loss: 1.1973 - val_loss: 1.5592 - val_mean_absolute_error: 12.3898 - val_mean_squared_error: 479.4992 - val__pinball_loss: 1.5592\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1908 - mean_absolute_error: 9.7592 - mean_squared_error: 341.1013 - _pinball_loss: 1.1908 - val_loss: 1.5536 - val_mean_absolute_error: 12.3858 - val_mean_squared_error: 474.9269 - val__pinball_loss: 1.5536\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1850 - mean_absolute_error: 9.7101 - mean_squared_error: 338.9691 - _pinball_loss: 1.1850 - val_loss: 1.5590 - val_mean_absolute_error: 12.7964 - val_mean_squared_error: 502.6900 - val__pinball_loss: 1.5590\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1792 - mean_absolute_error: 9.6721 - mean_squared_error: 337.8264 - _pinball_loss: 1.1792 - val_loss: 1.5563 - val_mean_absolute_error: 12.3402 - val_mean_squared_error: 475.9934 - val__pinball_loss: 1.5563\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1754 - mean_absolute_error: 9.6532 - mean_squared_error: 337.2619 - _pinball_loss: 1.1754 - val_loss: 1.5459 - val_mean_absolute_error: 12.3764 - val_mean_squared_error: 484.8332 - val__pinball_loss: 1.5459\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1693 - mean_absolute_error: 9.5936 - mean_squared_error: 334.2650 - _pinball_loss: 1.1693 - val_loss: 1.5632 - val_mean_absolute_error: 11.8496 - val_mean_squared_error: 449.3615 - val__pinball_loss: 1.5632\n",
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1670 - mean_absolute_error: 9.5545 - mean_squared_error: 332.1129 - _pinball_loss: 1.1670 - val_loss: 1.5475 - val_mean_absolute_error: 12.3463 - val_mean_squared_error: 475.6871 - val__pinball_loss: 1.5475\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1613 - mean_absolute_error: 9.5272 - mean_squared_error: 331.0920 - _pinball_loss: 1.1613 - val_loss: 1.5426 - val_mean_absolute_error: 12.5277 - val_mean_squared_error: 487.3673 - val__pinball_loss: 1.5426\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1572 - mean_absolute_error: 9.4948 - mean_squared_error: 329.7560 - _pinball_loss: 1.1572 - val_loss: 1.5416 - val_mean_absolute_error: 12.4091 - val_mean_squared_error: 488.8802 - val__pinball_loss: 1.5416\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1539 - mean_absolute_error: 9.4633 - mean_squared_error: 328.1390 - _pinball_loss: 1.1539 - val_loss: 1.5369 - val_mean_absolute_error: 12.2544 - val_mean_squared_error: 473.6070 - val__pinball_loss: 1.5369\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1474 - mean_absolute_error: 9.4069 - mean_squared_error: 325.5197 - _pinball_loss: 1.1474 - val_loss: 1.5518 - val_mean_absolute_error: 11.8083 - val_mean_squared_error: 447.1150 - val__pinball_loss: 1.5518\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1447 - mean_absolute_error: 9.3719 - mean_squared_error: 323.7652 - _pinball_loss: 1.1447 - val_loss: 1.5318 - val_mean_absolute_error: 12.4882 - val_mean_squared_error: 491.2029 - val__pinball_loss: 1.5318\n",
      "Epoch 21/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.1404 - mean_absolute_error: 9.3550 - mean_squared_error: 323.1269 - _pinball_loss: 1.1404 - val_loss: 1.5428 - val_mean_absolute_error: 11.8752 - val_mean_squared_error: 451.9731 - val__pinball_loss: 1.5428\n",
      "Epoch 22/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1390 - mean_absolute_error: 9.3356 - mean_squared_error: 321.9957 - _pinball_loss: 1.1390 - val_loss: 1.5333 - val_mean_absolute_error: 11.8237 - val_mean_squared_error: 447.8629 - val__pinball_loss: 1.5333\n",
      "Epoch 23/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1324 - mean_absolute_error: 9.2810 - mean_squared_error: 319.8773 - _pinball_loss: 1.1324 - val_loss: 1.5311 - val_mean_absolute_error: 12.3723 - val_mean_squared_error: 483.9882 - val__pinball_loss: 1.5311\n",
      "Epoch 24/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1296 - mean_absolute_error: 9.2502 - mean_squared_error: 317.3194 - _pinball_loss: 1.1296 - val_loss: 1.5313 - val_mean_absolute_error: 11.9829 - val_mean_squared_error: 460.0618 - val__pinball_loss: 1.5313\n",
      "Epoch 25/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1257 - mean_absolute_error: 9.2137 - mean_squared_error: 315.8439 - _pinball_loss: 1.1257 - val_loss: 1.5376 - val_mean_absolute_error: 11.8525 - val_mean_squared_error: 451.9845 - val__pinball_loss: 1.5376\n",
      "Epoch 26/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.1210 - mean_absolute_error: 9.1574 - mean_squared_error: 312.9481 - _pinball_loss: 1.1210 - val_loss: 1.5403 - val_mean_absolute_error: 12.0412 - val_mean_squared_error: 463.9633 - val__pinball_loss: 1.5403\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000267816A8F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 2.6045 - mean_absolute_error: 10.9928 - mean_squared_error: 406.7802 - _pinball_loss: 2.6045 - val_loss: 2.7202 - val_mean_absolute_error: 9.6590 - val_mean_squared_error: 278.3925 - val__pinball_loss: 2.7202\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 2.0939 - mean_absolute_error: 7.8009 - mean_squared_error: 205.6968 - _pinball_loss: 2.0939 - val_loss: 2.6421 - val_mean_absolute_error: 9.5834 - val_mean_squared_error: 279.3486 - val__pinball_loss: 2.6421\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 2.0289 - mean_absolute_error: 7.5192 - mean_squared_error: 198.3588 - _pinball_loss: 2.0289 - val_loss: 2.6058 - val_mean_absolute_error: 9.6657 - val_mean_squared_error: 287.0864 - val__pinball_loss: 2.6058\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9991 - mean_absolute_error: 7.4039 - mean_squared_error: 195.7076 - _pinball_loss: 1.9991 - val_loss: 2.5546 - val_mean_absolute_error: 9.3270 - val_mean_squared_error: 275.0760 - val__pinball_loss: 2.5546\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9707 - mean_absolute_error: 7.2776 - mean_squared_error: 192.3399 - _pinball_loss: 1.9707 - val_loss: 2.5359 - val_mean_absolute_error: 9.0735 - val_mean_squared_error: 266.4058 - val__pinball_loss: 2.5359\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9520 - mean_absolute_error: 7.2105 - mean_squared_error: 190.6654 - _pinball_loss: 1.9520 - val_loss: 2.5000 - val_mean_absolute_error: 9.2299 - val_mean_squared_error: 276.4506 - val__pinball_loss: 2.5000\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9357 - mean_absolute_error: 7.1622 - mean_squared_error: 190.1814 - _pinball_loss: 1.9357 - val_loss: 2.4963 - val_mean_absolute_error: 9.3532 - val_mean_squared_error: 284.3462 - val__pinball_loss: 2.4963\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9217 - mean_absolute_error: 7.0998 - mean_squared_error: 187.9885 - _pinball_loss: 1.9217 - val_loss: 2.4876 - val_mean_absolute_error: 9.4526 - val_mean_squared_error: 290.5293 - val__pinball_loss: 2.4876\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9109 - mean_absolute_error: 7.0582 - mean_squared_error: 186.7677 - _pinball_loss: 1.9109 - val_loss: 2.4884 - val_mean_absolute_error: 8.7201 - val_mean_squared_error: 256.6643 - val__pinball_loss: 2.4884\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9023 - mean_absolute_error: 7.0137 - mean_squared_error: 185.4878 - _pinball_loss: 1.9023 - val_loss: 2.4541 - val_mean_absolute_error: 9.1804 - val_mean_squared_error: 278.8311 - val__pinball_loss: 2.4541\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8922 - mean_absolute_error: 6.9777 - mean_squared_error: 184.5911 - _pinball_loss: 1.8922 - val_loss: 2.4559 - val_mean_absolute_error: 9.3750 - val_mean_squared_error: 287.1664 - val__pinball_loss: 2.4559\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8846 - mean_absolute_error: 6.9587 - mean_squared_error: 184.3216 - _pinball_loss: 1.8846 - val_loss: 2.4514 - val_mean_absolute_error: 9.1875 - val_mean_squared_error: 284.0271 - val__pinball_loss: 2.4514\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8760 - mean_absolute_error: 6.9168 - mean_squared_error: 182.8774 - _pinball_loss: 1.8760 - val_loss: 2.4352 - val_mean_absolute_error: 8.7806 - val_mean_squared_error: 259.0623 - val__pinball_loss: 2.4352\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8675 - mean_absolute_error: 6.8859 - mean_squared_error: 181.7632 - _pinball_loss: 1.8675 - val_loss: 2.4302 - val_mean_absolute_error: 9.1326 - val_mean_squared_error: 278.3347 - val__pinball_loss: 2.4302\n",
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 1.8622 - mean_absolute_error: 6.8508 - mean_squared_error: 180.3194 - _pinball_loss: 1.8622 - val_loss: 2.4318 - val_mean_absolute_error: 9.0640 - val_mean_squared_error: 274.1381 - val__pinball_loss: 2.4318\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8562 - mean_absolute_error: 6.8370 - mean_squared_error: 179.6814 - _pinball_loss: 1.8562 - val_loss: 2.4313 - val_mean_absolute_error: 8.9804 - val_mean_squared_error: 271.5995 - val__pinball_loss: 2.4313\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8473 - mean_absolute_error: 6.7877 - mean_squared_error: 177.8088 - _pinball_loss: 1.8473 - val_loss: 2.4282 - val_mean_absolute_error: 9.0737 - val_mean_squared_error: 272.3094 - val__pinball_loss: 2.4282\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8391 - mean_absolute_error: 6.7669 - mean_squared_error: 177.1095 - _pinball_loss: 1.8391 - val_loss: 2.4199 - val_mean_absolute_error: 8.8080 - val_mean_squared_error: 262.2791 - val__pinball_loss: 2.4199\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8335 - mean_absolute_error: 6.7442 - mean_squared_error: 176.6241 - _pinball_loss: 1.8335 - val_loss: 2.4181 - val_mean_absolute_error: 8.7907 - val_mean_squared_error: 260.2855 - val__pinball_loss: 2.4181\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8260 - mean_absolute_error: 6.7122 - mean_squared_error: 175.3270 - _pinball_loss: 1.8260 - val_loss: 2.4251 - val_mean_absolute_error: 8.5067 - val_mean_squared_error: 249.2532 - val__pinball_loss: 2.4251\n",
      "Epoch 21/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8178 - mean_absolute_error: 6.6747 - mean_squared_error: 174.3109 - _pinball_loss: 1.8178 - val_loss: 2.4212 - val_mean_absolute_error: 8.7361 - val_mean_squared_error: 261.4553 - val__pinball_loss: 2.4212\n",
      "Epoch 22/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8120 - mean_absolute_error: 6.6596 - mean_squared_error: 173.1317 - _pinball_loss: 1.8120 - val_loss: 2.4142 - val_mean_absolute_error: 8.8076 - val_mean_squared_error: 266.5725 - val__pinball_loss: 2.4142\n",
      "Epoch 23/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8064 - mean_absolute_error: 6.6570 - mean_squared_error: 173.9667 - _pinball_loss: 1.8064 - val_loss: 2.4098 - val_mean_absolute_error: 8.6748 - val_mean_squared_error: 254.4124 - val__pinball_loss: 2.4098\n",
      "Epoch 24/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8008 - mean_absolute_error: 6.6178 - mean_squared_error: 171.8236 - _pinball_loss: 1.8008 - val_loss: 2.4243 - val_mean_absolute_error: 8.8667 - val_mean_squared_error: 267.0129 - val__pinball_loss: 2.4243\n",
      "Epoch 25/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.7960 - mean_absolute_error: 6.6157 - mean_squared_error: 171.9695 - _pinball_loss: 1.7960 - val_loss: 2.4199 - val_mean_absolute_error: 8.5215 - val_mean_squared_error: 250.2938 - val__pinball_loss: 2.4199\n",
      "Epoch 26/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.7855 - mean_absolute_error: 6.5708 - mean_squared_error: 170.7061 - _pinball_loss: 1.7855 - val_loss: 2.4226 - val_mean_absolute_error: 8.7689 - val_mean_squared_error: 261.5353 - val__pinball_loss: 2.4226\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000269F5CC75E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 3.3975 - mean_absolute_error: 9.4513 - mean_squared_error: 314.3611 - _pinball_loss: 3.3975 - val_loss: 3.2831 - val_mean_absolute_error: 8.0008 - val_mean_squared_error: 197.7634 - val__pinball_loss: 3.2831\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.5413 - mean_absolute_error: 6.2413 - mean_squared_error: 137.6738 - _pinball_loss: 2.5413 - val_loss: 3.1681 - val_mean_absolute_error: 7.5629 - val_mean_squared_error: 187.2613 - val__pinball_loss: 3.1681\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4609 - mean_absolute_error: 6.0198 - mean_squared_error: 132.8116 - _pinball_loss: 2.4609 - val_loss: 3.1186 - val_mean_absolute_error: 7.5807 - val_mean_squared_error: 188.1870 - val__pinball_loss: 3.1186\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4115 - mean_absolute_error: 5.8882 - mean_squared_error: 129.9067 - _pinball_loss: 2.4115 - val_loss: 3.0647 - val_mean_absolute_error: 7.3704 - val_mean_squared_error: 184.5678 - val__pinball_loss: 3.0647\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3793 - mean_absolute_error: 5.8097 - mean_squared_error: 128.1167 - _pinball_loss: 2.3793 - val_loss: 3.0215 - val_mean_absolute_error: 7.3786 - val_mean_squared_error: 184.4928 - val__pinball_loss: 3.0215\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3527 - mean_absolute_error: 5.7482 - mean_squared_error: 126.8111 - _pinball_loss: 2.3527 - val_loss: 3.0381 - val_mean_absolute_error: 7.1962 - val_mean_squared_error: 180.9523 - val__pinball_loss: 3.0381\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3334 - mean_absolute_error: 5.6877 - mean_squared_error: 125.3611 - _pinball_loss: 2.3334 - val_loss: 3.0392 - val_mean_absolute_error: 7.1523 - val_mean_squared_error: 178.6919 - val__pinball_loss: 3.0392\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 2.3201 - mean_absolute_error: 5.6603 - mean_squared_error: 124.6648 - _pinball_loss: 2.3201 - val_loss: 2.9856 - val_mean_absolute_error: 7.1075 - val_mean_squared_error: 176.7619 - val__pinball_loss: 2.9856\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3035 - mean_absolute_error: 5.6073 - mean_squared_error: 123.3669 - _pinball_loss: 2.3035 - val_loss: 2.9616 - val_mean_absolute_error: 7.0836 - val_mean_squared_error: 176.0862 - val__pinball_loss: 2.9616\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2903 - mean_absolute_error: 5.5873 - mean_squared_error: 122.7867 - _pinball_loss: 2.2903 - val_loss: 2.9740 - val_mean_absolute_error: 7.1035 - val_mean_squared_error: 177.0668 - val__pinball_loss: 2.9740\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2803 - mean_absolute_error: 5.5530 - mean_squared_error: 121.9201 - _pinball_loss: 2.2803 - val_loss: 2.9686 - val_mean_absolute_error: 7.3799 - val_mean_squared_error: 184.1653 - val__pinball_loss: 2.9686\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2679 - mean_absolute_error: 5.5322 - mean_squared_error: 121.5696 - _pinball_loss: 2.2679 - val_loss: 2.9440 - val_mean_absolute_error: 7.0551 - val_mean_squared_error: 175.4521 - val__pinball_loss: 2.9440\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2581 - mean_absolute_error: 5.4938 - mean_squared_error: 120.4070 - _pinball_loss: 2.2581 - val_loss: 2.9591 - val_mean_absolute_error: 7.1519 - val_mean_squared_error: 179.2772 - val__pinball_loss: 2.9591\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 2.2474 - mean_absolute_error: 5.4706 - mean_squared_error: 119.9465 - _pinball_loss: 2.2474 - val_loss: 2.9400 - val_mean_absolute_error: 7.3240 - val_mean_squared_error: 183.7836 - val__pinball_loss: 2.9400\n",
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2405 - mean_absolute_error: 5.4533 - mean_squared_error: 119.5896 - _pinball_loss: 2.2405 - val_loss: 2.9307 - val_mean_absolute_error: 6.8363 - val_mean_squared_error: 169.8284 - val__pinball_loss: 2.9307\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2313 - mean_absolute_error: 5.4163 - mean_squared_error: 118.4213 - _pinball_loss: 2.2313 - val_loss: 2.9208 - val_mean_absolute_error: 6.8749 - val_mean_squared_error: 172.1979 - val__pinball_loss: 2.9208\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 2.2200 - mean_absolute_error: 5.3949 - mean_squared_error: 117.9304 - _pinball_loss: 2.2200 - val_loss: 2.9236 - val_mean_absolute_error: 6.9842 - val_mean_squared_error: 175.7980 - val__pinball_loss: 2.9236\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 2.2140 - mean_absolute_error: 5.3831 - mean_squared_error: 117.5591 - _pinball_loss: 2.2140 - val_loss: 2.9173 - val_mean_absolute_error: 6.9445 - val_mean_squared_error: 174.9252 - val__pinball_loss: 2.9173\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2080 - mean_absolute_error: 5.3711 - mean_squared_error: 117.3685 - _pinball_loss: 2.2080 - val_loss: 2.9096 - val_mean_absolute_error: 7.3469 - val_mean_squared_error: 187.0658 - val__pinball_loss: 2.9096\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1994 - mean_absolute_error: 5.3573 - mean_squared_error: 116.9636 - _pinball_loss: 2.1994 - val_loss: 2.9339 - val_mean_absolute_error: 6.8600 - val_mean_squared_error: 172.0595 - val__pinball_loss: 2.9339\n",
      "Epoch 21/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1929 - mean_absolute_error: 5.3305 - mean_squared_error: 116.1739 - _pinball_loss: 2.1929 - val_loss: 2.9163 - val_mean_absolute_error: 7.1287 - val_mean_squared_error: 180.2412 - val__pinball_loss: 2.9163\n",
      "Epoch 22/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1855 - mean_absolute_error: 5.3183 - mean_squared_error: 115.8191 - _pinball_loss: 2.1855 - val_loss: 2.9204 - val_mean_absolute_error: 6.7437 - val_mean_squared_error: 170.2689 - val__pinball_loss: 2.9204\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000267899CEC18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.4\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 3.9516 - mean_absolute_error: 8.7337 - mean_squared_error: 272.0300 - _pinball_loss: 3.9516 - val_loss: 3.5018 - val_mean_absolute_error: 7.2414 - val_mean_squared_error: 178.1272 - val__pinball_loss: 3.5018\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.6996 - mean_absolute_error: 5.5148 - mean_squared_error: 119.3915 - _pinball_loss: 2.6996 - val_loss: 3.3829 - val_mean_absolute_error: 6.9514 - val_mean_squared_error: 170.2863 - val__pinball_loss: 3.3829\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.6095 - mean_absolute_error: 5.3156 - mean_squared_error: 114.9513 - _pinball_loss: 2.6095 - val_loss: 3.3362 - val_mean_absolute_error: 6.8706 - val_mean_squared_error: 167.0997 - val__pinball_loss: 3.3362\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.5538 - mean_absolute_error: 5.1962 - mean_squared_error: 112.2186 - _pinball_loss: 2.5538 - val_loss: 3.2759 - val_mean_absolute_error: 6.7392 - val_mean_squared_error: 164.1526 - val__pinball_loss: 3.2759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.5177 - mean_absolute_error: 5.1217 - mean_squared_error: 110.1409 - _pinball_loss: 2.5177 - val_loss: 3.2501 - val_mean_absolute_error: 6.6501 - val_mean_squared_error: 162.0579 - val__pinball_loss: 3.2501\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4893 - mean_absolute_error: 5.0559 - mean_squared_error: 108.6293 - _pinball_loss: 2.4893 - val_loss: 3.2288 - val_mean_absolute_error: 6.6059 - val_mean_squared_error: 160.7237 - val__pinball_loss: 3.2288\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4673 - mean_absolute_error: 5.0106 - mean_squared_error: 107.3469 - _pinball_loss: 2.4673 - val_loss: 3.1829 - val_mean_absolute_error: 6.5676 - val_mean_squared_error: 159.6398 - val__pinball_loss: 3.1829\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4442 - mean_absolute_error: 4.9609 - mean_squared_error: 106.1783 - _pinball_loss: 2.4442 - val_loss: 3.1763 - val_mean_absolute_error: 6.5381 - val_mean_squared_error: 158.8288 - val__pinball_loss: 3.1763\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4304 - mean_absolute_error: 4.9310 - mean_squared_error: 105.3758 - _pinball_loss: 2.4304 - val_loss: 3.1452 - val_mean_absolute_error: 6.4879 - val_mean_squared_error: 157.2439 - val__pinball_loss: 3.1452\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4122 - mean_absolute_error: 4.8926 - mean_squared_error: 104.5429 - _pinball_loss: 2.4122 - val_loss: 3.1392 - val_mean_absolute_error: 6.4917 - val_mean_squared_error: 156.4787 - val__pinball_loss: 3.1392\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3974 - mean_absolute_error: 4.8630 - mean_squared_error: 103.9097 - _pinball_loss: 2.3974 - val_loss: 3.1310 - val_mean_absolute_error: 6.4128 - val_mean_squared_error: 156.0146 - val__pinball_loss: 3.1310\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3867 - mean_absolute_error: 4.8374 - mean_squared_error: 103.2722 - _pinball_loss: 2.3867 - val_loss: 3.1110 - val_mean_absolute_error: 6.3956 - val_mean_squared_error: 154.6611 - val__pinball_loss: 3.1110\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3757 - mean_absolute_error: 4.8140 - mean_squared_error: 102.8769 - _pinball_loss: 2.3757 - val_loss: 3.0984 - val_mean_absolute_error: 6.2490 - val_mean_squared_error: 155.2173 - val__pinball_loss: 3.0984\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3653 - mean_absolute_error: 4.7921 - mean_squared_error: 102.3368 - _pinball_loss: 2.3653 - val_loss: 3.1062 - val_mean_absolute_error: 6.3649 - val_mean_squared_error: 155.2272 - val__pinball_loss: 3.1062\n",
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3578 - mean_absolute_error: 4.7781 - mean_squared_error: 101.9380 - _pinball_loss: 2.3578 - val_loss: 3.1147 - val_mean_absolute_error: 6.3517 - val_mean_squared_error: 154.0798 - val__pinball_loss: 3.1147\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3462 - mean_absolute_error: 4.7491 - mean_squared_error: 101.2676 - _pinball_loss: 2.3462 - val_loss: 3.1027 - val_mean_absolute_error: 6.3523 - val_mean_squared_error: 154.9649 - val__pinball_loss: 3.1027\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026A3A896AF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.5\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 4.1777 - mean_absolute_error: 8.3553 - mean_squared_error: 246.9445 - _pinball_loss: 4.1777 - val_loss: 3.4343 - val_mean_absolute_error: 6.8685 - val_mean_squared_error: 184.6454 - val__pinball_loss: 3.4343\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.6631 - mean_absolute_error: 5.3261 - mean_squared_error: 123.7169 - _pinball_loss: 2.6631 - val_loss: 3.3745 - val_mean_absolute_error: 6.7489 - val_mean_squared_error: 176.8571 - val__pinball_loss: 3.3745\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.5680 - mean_absolute_error: 5.1360 - mean_squared_error: 119.6372 - _pinball_loss: 2.5680 - val_loss: 3.2566 - val_mean_absolute_error: 6.5133 - val_mean_squared_error: 168.5129 - val__pinball_loss: 3.2566\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.5125 - mean_absolute_error: 5.0250 - mean_squared_error: 117.1834 - _pinball_loss: 2.5125 - val_loss: 3.2414 - val_mean_absolute_error: 6.4829 - val_mean_squared_error: 167.1075 - val__pinball_loss: 3.2414\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4714 - mean_absolute_error: 4.9429 - mean_squared_error: 115.2757 - _pinball_loss: 2.4714 - val_loss: 3.1638 - val_mean_absolute_error: 6.3275 - val_mean_squared_error: 169.9989 - val__pinball_loss: 3.1638\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4455 - mean_absolute_error: 4.8910 - mean_squared_error: 113.9931 - _pinball_loss: 2.4455 - val_loss: 3.1435 - val_mean_absolute_error: 6.2870 - val_mean_squared_error: 167.0947 - val__pinball_loss: 3.1435\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4148 - mean_absolute_error: 4.8296 - mean_squared_error: 112.6255 - _pinball_loss: 2.4148 - val_loss: 3.1557 - val_mean_absolute_error: 6.3115 - val_mean_squared_error: 162.5967 - val__pinball_loss: 3.1557\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3971 - mean_absolute_error: 4.7942 - mean_squared_error: 111.6269 - _pinball_loss: 2.3971 - val_loss: 3.1131 - val_mean_absolute_error: 6.2262 - val_mean_squared_error: 165.4009 - val__pinball_loss: 3.1131\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3803 - mean_absolute_error: 4.7606 - mean_squared_error: 111.0268 - _pinball_loss: 2.3803 - val_loss: 3.0864 - val_mean_absolute_error: 6.1727 - val_mean_squared_error: 163.2530 - val__pinball_loss: 3.0864\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3654 - mean_absolute_error: 4.7308 - mean_squared_error: 110.2962 - _pinball_loss: 2.3654 - val_loss: 3.0730 - val_mean_absolute_error: 6.1461 - val_mean_squared_error: 160.4391 - val__pinball_loss: 3.0730\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3501 - mean_absolute_error: 4.7003 - mean_squared_error: 109.6970 - _pinball_loss: 2.3501 - val_loss: 3.0615 - val_mean_absolute_error: 6.1231 - val_mean_squared_error: 157.9790 - val__pinball_loss: 3.0615\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.3303 - mean_absolute_error: 4.6607 - mean_squared_error: 108.8256 - _pinball_loss: 2.3303 - val_loss: 3.0629 - val_mean_absolute_error: 6.1259 - val_mean_squared_error: 166.0853 - val__pinball_loss: 3.0629\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3256 - mean_absolute_error: 4.6513 - mean_squared_error: 108.5449 - _pinball_loss: 2.3256 - val_loss: 3.0247 - val_mean_absolute_error: 6.0494 - val_mean_squared_error: 161.9604 - val__pinball_loss: 3.0247\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.3115 - mean_absolute_error: 4.6231 - mean_squared_error: 107.9848 - _pinball_loss: 2.3115 - val_loss: 3.0290 - val_mean_absolute_error: 6.0580 - val_mean_squared_error: 160.3078 - val__pinball_loss: 3.0290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.2988 - mean_absolute_error: 4.5976 - mean_squared_error: 107.2081 - _pinball_loss: 2.2988 - val_loss: 3.0302 - val_mean_absolute_error: 6.0604 - val_mean_squared_error: 164.8270 - val__pinball_loss: 3.0302\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.2903 - mean_absolute_error: 4.5807 - mean_squared_error: 106.8034 - _pinball_loss: 2.2903 - val_loss: 3.0170 - val_mean_absolute_error: 6.0340 - val_mean_squared_error: 162.1796 - val__pinball_loss: 3.0170\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.2797 - mean_absolute_error: 4.5593 - mean_squared_error: 106.5370 - _pinball_loss: 2.2797 - val_loss: 2.9969 - val_mean_absolute_error: 5.9938 - val_mean_squared_error: 160.8394 - val__pinball_loss: 2.9969\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2692 - mean_absolute_error: 4.5385 - mean_squared_error: 105.9044 - _pinball_loss: 2.2692 - val_loss: 3.0098 - val_mean_absolute_error: 6.0196 - val_mean_squared_error: 165.1266 - val__pinball_loss: 3.0098\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2613 - mean_absolute_error: 4.5227 - mean_squared_error: 105.6536 - _pinball_loss: 2.2613 - val_loss: 3.0035 - val_mean_absolute_error: 6.0070 - val_mean_squared_error: 158.0494 - val__pinball_loss: 3.0035\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2514 - mean_absolute_error: 4.5028 - mean_squared_error: 104.9794 - _pinball_loss: 2.2514 - val_loss: 3.0133 - val_mean_absolute_error: 6.0265 - val_mean_squared_error: 164.0970 - val__pinball_loss: 3.0133\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000267816A8288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.6\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 4.3871 - mean_absolute_error: 8.8801 - mean_squared_error: 257.1516 - _pinball_loss: 4.3871 - val_loss: 3.2028 - val_mean_absolute_error: 7.0042 - val_mean_squared_error: 197.8118 - val__pinball_loss: 3.2028\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.4699 - mean_absolute_error: 5.4772 - mean_squared_error: 136.5365 - _pinball_loss: 2.4699 - val_loss: 3.0663 - val_mean_absolute_error: 6.6924 - val_mean_squared_error: 192.4663 - val__pinball_loss: 3.0663\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.3539 - mean_absolute_error: 5.2441 - mean_squared_error: 132.8681 - _pinball_loss: 2.3539 - val_loss: 3.0720 - val_mean_absolute_error: 6.6157 - val_mean_squared_error: 183.4514 - val__pinball_loss: 3.0720\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.2986 - mean_absolute_error: 5.1367 - mean_squared_error: 130.9813 - _pinball_loss: 2.2986 - val_loss: 2.9488 - val_mean_absolute_error: 6.4940 - val_mean_squared_error: 188.4846 - val__pinball_loss: 2.9488\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2554 - mean_absolute_error: 5.0523 - mean_squared_error: 129.7233 - _pinball_loss: 2.2554 - val_loss: 2.8867 - val_mean_absolute_error: 6.3582 - val_mean_squared_error: 185.0540 - val__pinball_loss: 2.8867\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.2237 - mean_absolute_error: 4.9899 - mean_squared_error: 128.3864 - _pinball_loss: 2.2237 - val_loss: 2.8768 - val_mean_absolute_error: 6.3997 - val_mean_squared_error: 190.3047 - val__pinball_loss: 2.8768\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1958 - mean_absolute_error: 4.9360 - mean_squared_error: 127.6050 - _pinball_loss: 2.1958 - val_loss: 2.8274 - val_mean_absolute_error: 6.3378 - val_mean_squared_error: 189.5997 - val__pinball_loss: 2.8274\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1772 - mean_absolute_error: 4.9023 - mean_squared_error: 127.1105 - _pinball_loss: 2.1772 - val_loss: 2.8329 - val_mean_absolute_error: 6.2572 - val_mean_squared_error: 181.9863 - val__pinball_loss: 2.8329\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1566 - mean_absolute_error: 4.8592 - mean_squared_error: 126.2391 - _pinball_loss: 2.1566 - val_loss: 2.8173 - val_mean_absolute_error: 6.2525 - val_mean_squared_error: 182.6884 - val__pinball_loss: 2.8173\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.1360 - mean_absolute_error: 4.8195 - mean_squared_error: 125.7114 - _pinball_loss: 2.1360 - val_loss: 2.7730 - val_mean_absolute_error: 6.2545 - val_mean_squared_error: 187.7830 - val__pinball_loss: 2.7730\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.1224 - mean_absolute_error: 4.7964 - mean_squared_error: 125.5456 - _pinball_loss: 2.1224 - val_loss: 2.7719 - val_mean_absolute_error: 6.2632 - val_mean_squared_error: 189.0852 - val__pinball_loss: 2.7719\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.1091 - mean_absolute_error: 4.7669 - mean_squared_error: 124.6188 - _pinball_loss: 2.1091 - val_loss: 2.7967 - val_mean_absolute_error: 6.3315 - val_mean_squared_error: 189.5202 - val__pinball_loss: 2.7967\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.1002 - mean_absolute_error: 4.7517 - mean_squared_error: 124.6055 - _pinball_loss: 2.1002 - val_loss: 2.7569 - val_mean_absolute_error: 6.1676 - val_mean_squared_error: 182.4693 - val__pinball_loss: 2.7569\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.0884 - mean_absolute_error: 4.7258 - mean_squared_error: 123.8808 - _pinball_loss: 2.0884 - val_loss: 2.7663 - val_mean_absolute_error: 6.1447 - val_mean_squared_error: 182.3062 - val__pinball_loss: 2.7663\n",
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.0771 - mean_absolute_error: 4.7026 - mean_squared_error: 123.4981 - _pinball_loss: 2.0771 - val_loss: 2.7474 - val_mean_absolute_error: 6.2177 - val_mean_squared_error: 190.9944 - val__pinball_loss: 2.7474\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 2.0674 - mean_absolute_error: 4.6843 - mean_squared_error: 123.1039 - _pinball_loss: 2.0674 - val_loss: 2.7376 - val_mean_absolute_error: 6.2773 - val_mean_squared_error: 197.3357 - val__pinball_loss: 2.7376\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.0552 - mean_absolute_error: 4.6615 - mean_squared_error: 122.9385 - _pinball_loss: 2.0552 - val_loss: 2.7164 - val_mean_absolute_error: 6.1253 - val_mean_squared_error: 186.6932 - val__pinball_loss: 2.7164\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.0442 - mean_absolute_error: 4.6368 - mean_squared_error: 122.1202 - _pinball_loss: 2.0442 - val_loss: 2.7375 - val_mean_absolute_error: 6.1125 - val_mean_squared_error: 183.1403 - val__pinball_loss: 2.7375\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.0382 - mean_absolute_error: 4.6243 - mean_squared_error: 121.8330 - _pinball_loss: 2.0382 - val_loss: 2.7319 - val_mean_absolute_error: 6.1602 - val_mean_squared_error: 187.7780 - val__pinball_loss: 2.7319\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.0319 - mean_absolute_error: 4.6097 - mean_squared_error: 121.4264 - _pinball_loss: 2.0319 - val_loss: 2.7150 - val_mean_absolute_error: 6.0489 - val_mean_squared_error: 181.2214 - val__pinball_loss: 2.7150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 2.0217 - mean_absolute_error: 4.5894 - mean_squared_error: 121.0099 - _pinball_loss: 2.0217 - val_loss: 2.7178 - val_mean_absolute_error: 6.1806 - val_mean_squared_error: 189.4162 - val__pinball_loss: 2.7178\n",
      "Epoch 22/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.0152 - mean_absolute_error: 4.5762 - mean_squared_error: 120.6292 - _pinball_loss: 2.0152 - val_loss: 2.7053 - val_mean_absolute_error: 6.0905 - val_mean_squared_error: 184.4009 - val__pinball_loss: 2.7053\n",
      "Epoch 23/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.0037 - mean_absolute_error: 4.5517 - mean_squared_error: 120.0696 - _pinball_loss: 2.0037 - val_loss: 2.6973 - val_mean_absolute_error: 6.1488 - val_mean_squared_error: 191.1049 - val__pinball_loss: 2.6973\n",
      "Epoch 24/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9982 - mean_absolute_error: 4.5414 - mean_squared_error: 119.9745 - _pinball_loss: 1.9982 - val_loss: 2.7107 - val_mean_absolute_error: 6.1077 - val_mean_squared_error: 185.5687 - val__pinball_loss: 2.7107\n",
      "Epoch 25/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9989 - mean_absolute_error: 4.5370 - mean_squared_error: 119.2114 - _pinball_loss: 1.9989 - val_loss: 2.6996 - val_mean_absolute_error: 6.0737 - val_mean_squared_error: 185.0783 - val__pinball_loss: 2.6996\n",
      "Epoch 26/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9844 - mean_absolute_error: 4.5117 - mean_squared_error: 119.1005 - _pinball_loss: 1.9844 - val_loss: 2.6890 - val_mean_absolute_error: 6.0783 - val_mean_squared_error: 185.1681 - val__pinball_loss: 2.6890\n",
      "Epoch 27/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9825 - mean_absolute_error: 4.5066 - mean_squared_error: 118.8092 - _pinball_loss: 1.9825 - val_loss: 2.7105 - val_mean_absolute_error: 6.0482 - val_mean_squared_error: 182.2280 - val__pinball_loss: 2.7105\n",
      "Epoch 28/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 1.9768 - mean_absolute_error: 4.4926 - mean_squared_error: 118.2310 - _pinball_loss: 1.9768 - val_loss: 2.6995 - val_mean_absolute_error: 6.0410 - val_mean_squared_error: 179.0440 - val__pinball_loss: 2.6995\n",
      "Epoch 29/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9717 - mean_absolute_error: 4.4806 - mean_squared_error: 117.6990 - _pinball_loss: 1.9717 - val_loss: 2.6805 - val_mean_absolute_error: 6.0633 - val_mean_squared_error: 185.3777 - val__pinball_loss: 2.6805\n",
      "Epoch 30/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 1.9664 - mean_absolute_error: 4.4694 - mean_squared_error: 117.3767 - _pinball_loss: 1.9664 - val_loss: 2.6922 - val_mean_absolute_error: 5.9789 - val_mean_squared_error: 177.7341 - val__pinball_loss: 2.6922\n",
      "Epoch 31/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 1.9611 - mean_absolute_error: 4.4572 - mean_squared_error: 116.9044 - _pinball_loss: 1.9611 - val_loss: 2.6829 - val_mean_absolute_error: 6.0505 - val_mean_squared_error: 185.6910 - val__pinball_loss: 2.6829\n",
      "Epoch 32/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9520 - mean_absolute_error: 4.4407 - mean_squared_error: 116.7144 - _pinball_loss: 1.9520 - val_loss: 2.6874 - val_mean_absolute_error: 6.0954 - val_mean_squared_error: 187.8825 - val__pinball_loss: 2.6874\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026A43E48948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.7\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 10s 8ms/step - loss: 4.4861 - mean_absolute_error: 10.3755 - mean_squared_error: 293.4345 - _pinball_loss: 4.4861 - val_loss: 2.7622 - val_mean_absolute_error: 7.5561 - val_mean_squared_error: 230.7968 - val__pinball_loss: 2.7622\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.1704 - mean_absolute_error: 6.0059 - mean_squared_error: 158.2348 - _pinball_loss: 2.1704 - val_loss: 2.5885 - val_mean_absolute_error: 7.0823 - val_mean_squared_error: 221.1999 - val__pinball_loss: 2.5885\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 2.0407 - mean_absolute_error: 5.7083 - mean_squared_error: 154.6252 - _pinball_loss: 2.0407 - val_loss: 2.4927 - val_mean_absolute_error: 6.8868 - val_mean_squared_error: 218.3573 - val__pinball_loss: 2.4927\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9802 - mean_absolute_error: 5.5776 - mean_squared_error: 153.4937 - _pinball_loss: 1.9802 - val_loss: 2.4937 - val_mean_absolute_error: 6.9155 - val_mean_squared_error: 214.6703 - val__pinball_loss: 2.4937\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.9384 - mean_absolute_error: 5.4906 - mean_squared_error: 152.4645 - _pinball_loss: 1.9384 - val_loss: 2.4543 - val_mean_absolute_error: 7.1586 - val_mean_squared_error: 233.9495 - val__pinball_loss: 2.4543\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 9s 8ms/step - loss: 1.9044 - mean_absolute_error: 5.4202 - mean_squared_error: 151.8717 - _pinball_loss: 1.9044 - val_loss: 2.3880 - val_mean_absolute_error: 6.7504 - val_mean_squared_error: 220.7026 - val__pinball_loss: 2.3880\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 10s 9ms/step - loss: 1.8748 - mean_absolute_error: 5.3576 - mean_squared_error: 151.0219 - _pinball_loss: 1.8748 - val_loss: 2.3903 - val_mean_absolute_error: 7.0353 - val_mean_squared_error: 236.7123 - val__pinball_loss: 2.3903\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 56s 49ms/step - loss: 1.8562 - mean_absolute_error: 5.3170 - mean_squared_error: 150.3148 - _pinball_loss: 1.8562 - val_loss: 2.3606 - val_mean_absolute_error: 6.9523 - val_mean_squared_error: 234.2416 - val__pinball_loss: 2.3606\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 341s 298ms/step - loss: 1.8349 - mean_absolute_error: 5.2736 - mean_squared_error: 149.9428 - _pinball_loss: 1.8349 - val_loss: 2.3378 - val_mean_absolute_error: 6.8595 - val_mean_squared_error: 231.6712 - val__pinball_loss: 2.3378\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 336s 294ms/step - loss: 1.8152 - mean_absolute_error: 5.2281 - mean_squared_error: 149.2150 - _pinball_loss: 1.8152 - val_loss: 2.3434 - val_mean_absolute_error: 6.7145 - val_mean_squared_error: 222.5712 - val__pinball_loss: 2.3434\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 337s 294ms/step - loss: 1.8051 - mean_absolute_error: 5.2046 - mean_squared_error: 148.7607 - _pinball_loss: 1.8051 - val_loss: 2.3248 - val_mean_absolute_error: 6.6595 - val_mean_squared_error: 219.1013 - val__pinball_loss: 2.3248\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 342s 298ms/step - loss: 1.7937 - mean_absolute_error: 5.1804 - mean_squared_error: 148.5253 - _pinball_loss: 1.7937 - val_loss: 2.3077 - val_mean_absolute_error: 6.6196 - val_mean_squared_error: 219.4037 - val__pinball_loss: 2.3077\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 349s 305ms/step - loss: 1.7810 - mean_absolute_error: 5.1528 - mean_squared_error: 148.2599 - _pinball_loss: 1.7810 - val_loss: 2.3264 - val_mean_absolute_error: 6.9680 - val_mean_squared_error: 239.3263 - val__pinball_loss: 2.3264\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 343s 300ms/step - loss: 1.7687 - mean_absolute_error: 5.1261 - mean_squared_error: 148.0053 - _pinball_loss: 1.7687 - val_loss: 2.2828 - val_mean_absolute_error: 6.7014 - val_mean_squared_error: 227.4557 - val__pinball_loss: 2.2828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 340s 297ms/step - loss: 1.7588 - mean_absolute_error: 5.1015 - mean_squared_error: 147.5875 - _pinball_loss: 1.7588 - val_loss: 2.2862 - val_mean_absolute_error: 6.6220 - val_mean_squared_error: 225.5252 - val__pinball_loss: 2.2862\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 347s 302ms/step - loss: 1.7531 - mean_absolute_error: 5.0881 - mean_squared_error: 147.2567 - _pinball_loss: 1.7531 - val_loss: 2.2639 - val_mean_absolute_error: 6.6967 - val_mean_squared_error: 228.3308 - val__pinball_loss: 2.2639\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 334s 291ms/step - loss: 1.7437 - mean_absolute_error: 5.0641 - mean_squared_error: 146.6617 - _pinball_loss: 1.7437 - val_loss: 2.2747 - val_mean_absolute_error: 6.6094 - val_mean_squared_error: 224.0950 - val__pinball_loss: 2.2747\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 335s 292ms/step - loss: 1.7334 - mean_absolute_error: 5.0397 - mean_squared_error: 146.2742 - _pinball_loss: 1.7334 - val_loss: 2.2516 - val_mean_absolute_error: 6.5302 - val_mean_squared_error: 219.1706 - val__pinball_loss: 2.2516\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 360s 314ms/step - loss: 1.7264 - mean_absolute_error: 5.0270 - mean_squared_error: 146.0949 - _pinball_loss: 1.7264 - val_loss: 2.2529 - val_mean_absolute_error: 6.5195 - val_mean_squared_error: 222.7088 - val__pinball_loss: 2.2529\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 305s 267ms/step - loss: 1.7161 - mean_absolute_error: 4.9977 - mean_squared_error: 145.3406 - _pinball_loss: 1.7161 - val_loss: 2.2598 - val_mean_absolute_error: 6.6278 - val_mean_squared_error: 223.5333 - val__pinball_loss: 2.2598\n",
      "Epoch 21/1000\n",
      "1146/1146 [==============================] - 263s 229ms/step - loss: 1.7145 - mean_absolute_error: 4.9925 - mean_squared_error: 145.0094 - _pinball_loss: 1.7145 - val_loss: 2.2465 - val_mean_absolute_error: 6.4790 - val_mean_squared_error: 219.1065 - val__pinball_loss: 2.2465\n",
      "Epoch 22/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 1.7062 - mean_absolute_error: 4.9693 - mean_squared_error: 144.3381 - _pinball_loss: 1.7062 - val_loss: 2.2499 - val_mean_absolute_error: 6.4086 - val_mean_squared_error: 214.4107 - val__pinball_loss: 2.2499\n",
      "Epoch 23/1000\n",
      "1146/1146 [==============================] - 266s 232ms/step - loss: 1.6999 - mean_absolute_error: 4.9578 - mean_squared_error: 144.2702 - _pinball_loss: 1.6999 - val_loss: 2.2451 - val_mean_absolute_error: 6.7075 - val_mean_squared_error: 231.3961 - val__pinball_loss: 2.2451\n",
      "Epoch 24/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 1.6930 - mean_absolute_error: 4.9427 - mean_squared_error: 143.9075 - _pinball_loss: 1.6930 - val_loss: 2.2304 - val_mean_absolute_error: 6.3907 - val_mean_squared_error: 214.9556 - val__pinball_loss: 2.2304\n",
      "Epoch 25/1000\n",
      "1146/1146 [==============================] - 286s 250ms/step - loss: 1.6891 - mean_absolute_error: 4.9317 - mean_squared_error: 143.3966 - _pinball_loss: 1.6891 - val_loss: 2.2400 - val_mean_absolute_error: 6.6334 - val_mean_squared_error: 226.7534 - val__pinball_loss: 2.2400\n",
      "Epoch 26/1000\n",
      "1146/1146 [==============================] - 341s 297ms/step - loss: 1.6834 - mean_absolute_error: 4.9153 - mean_squared_error: 142.7521 - _pinball_loss: 1.6834 - val_loss: 2.2389 - val_mean_absolute_error: 6.7438 - val_mean_squared_error: 231.7751 - val__pinball_loss: 2.2389\n",
      "Epoch 27/1000\n",
      "1146/1146 [==============================] - 263s 229ms/step - loss: 1.6817 - mean_absolute_error: 4.9121 - mean_squared_error: 142.6359 - _pinball_loss: 1.6817 - val_loss: 2.2505 - val_mean_absolute_error: 6.4272 - val_mean_squared_error: 214.4413 - val__pinball_loss: 2.2505\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026781A610D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.8\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 4.4923 - mean_absolute_error: 14.0404 - mean_squared_error: 449.0038 - _pinball_loss: 4.4923 - val_loss: 2.1886 - val_mean_absolute_error: 8.0475 - val_mean_squared_error: 254.6308 - val__pinball_loss: 2.1886\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 360s 314ms/step - loss: 1.7058 - mean_absolute_error: 6.8964 - mean_squared_error: 188.3062 - _pinball_loss: 1.7058 - val_loss: 1.9831 - val_mean_absolute_error: 7.8498 - val_mean_squared_error: 262.3571 - val__pinball_loss: 1.9831\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 366s 319ms/step - loss: 1.5783 - mean_absolute_error: 6.4937 - mean_squared_error: 182.7586 - _pinball_loss: 1.5783 - val_loss: 1.9338 - val_mean_absolute_error: 7.4807 - val_mean_squared_error: 247.8465 - val__pinball_loss: 1.9338\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 367s 320ms/step - loss: 1.5076 - mean_absolute_error: 6.2565 - mean_squared_error: 179.5408 - _pinball_loss: 1.5076 - val_loss: 1.8583 - val_mean_absolute_error: 7.4430 - val_mean_squared_error: 252.3376 - val__pinball_loss: 1.8583\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 270s 236ms/step - loss: 1.4611 - mean_absolute_error: 6.1108 - mean_squared_error: 177.5595 - _pinball_loss: 1.4611 - val_loss: 1.8317 - val_mean_absolute_error: 7.3348 - val_mean_squared_error: 250.3602 - val__pinball_loss: 1.8317\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 1.4301 - mean_absolute_error: 6.0164 - mean_squared_error: 176.2583 - _pinball_loss: 1.4301 - val_loss: 1.7764 - val_mean_absolute_error: 7.5433 - val_mean_squared_error: 263.9936 - val__pinball_loss: 1.7764\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 1.3975 - mean_absolute_error: 5.9206 - mean_squared_error: 175.2171 - _pinball_loss: 1.3975 - val_loss: 1.7641 - val_mean_absolute_error: 7.6688 - val_mean_squared_error: 268.8944 - val__pinball_loss: 1.7641\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 1.3775 - mean_absolute_error: 5.8643 - mean_squared_error: 174.5458 - _pinball_loss: 1.3775 - val_loss: 1.7594 - val_mean_absolute_error: 7.2183 - val_mean_squared_error: 253.8093 - val__pinball_loss: 1.7594\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 263s 230ms/step - loss: 1.3598 - mean_absolute_error: 5.7941 - mean_squared_error: 173.3421 - _pinball_loss: 1.3598 - val_loss: 1.7669 - val_mean_absolute_error: 7.1578 - val_mean_squared_error: 246.5345 - val__pinball_loss: 1.7669\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 264s 231ms/step - loss: 1.3447 - mean_absolute_error: 5.7530 - mean_squared_error: 173.0450 - _pinball_loss: 1.3447 - val_loss: 1.7878 - val_mean_absolute_error: 6.9239 - val_mean_squared_error: 241.5739 - val__pinball_loss: 1.7878\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 1.3360 - mean_absolute_error: 5.7236 - mean_squared_error: 172.6055 - _pinball_loss: 1.3360 - val_loss: 1.7011 - val_mean_absolute_error: 7.3263 - val_mean_squared_error: 261.7825 - val__pinball_loss: 1.7011\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 271s 237ms/step - loss: 1.3186 - mean_absolute_error: 5.6689 - mean_squared_error: 171.6092 - _pinball_loss: 1.3186 - val_loss: 1.7117 - val_mean_absolute_error: 7.0413 - val_mean_squared_error: 246.4352 - val__pinball_loss: 1.7117\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 264s 231ms/step - loss: 1.3099 - mean_absolute_error: 5.6428 - mean_squared_error: 171.3207 - _pinball_loss: 1.3099 - val_loss: 1.7038 - val_mean_absolute_error: 6.9687 - val_mean_squared_error: 245.1763 - val__pinball_loss: 1.7038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 262s 229ms/step - loss: 1.2992 - mean_absolute_error: 5.6056 - mean_squared_error: 170.7159 - _pinball_loss: 1.2992 - val_loss: 1.7018 - val_mean_absolute_error: 7.1433 - val_mean_squared_error: 253.1074 - val__pinball_loss: 1.7018\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000267816A8B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.9\n",
      "Epoch 1/1000\n",
      "1146/1146 [==============================] - 276s 241ms/step - loss: 3.7929 - mean_absolute_error: 21.3226 - mean_squared_error: 906.1551 - _pinball_loss: 3.7929 - val_loss: 1.3948 - val_mean_absolute_error: 9.8257 - val_mean_squared_error: 329.4081 - val__pinball_loss: 1.3948\n",
      "Epoch 2/1000\n",
      "1146/1146 [==============================] - 263s 229ms/step - loss: 1.0798 - mean_absolute_error: 8.8552 - mean_squared_error: 250.0107 - _pinball_loss: 1.0798 - val_loss: 1.1951 - val_mean_absolute_error: 9.2043 - val_mean_squared_error: 314.4237 - val__pinball_loss: 1.1951\n",
      "Epoch 3/1000\n",
      "1146/1146 [==============================] - 263s 229ms/step - loss: 0.9661 - mean_absolute_error: 8.0655 - mean_squared_error: 234.8569 - _pinball_loss: 0.9661 - val_loss: 1.1269 - val_mean_absolute_error: 8.6488 - val_mean_squared_error: 297.5486 - val__pinball_loss: 1.1269\n",
      "Epoch 4/1000\n",
      "1146/1146 [==============================] - 263s 229ms/step - loss: 0.9085 - mean_absolute_error: 7.6452 - mean_squared_error: 226.9658 - _pinball_loss: 0.9085 - val_loss: 1.0675 - val_mean_absolute_error: 8.7219 - val_mean_squared_error: 306.0605 - val__pinball_loss: 1.0675\n",
      "Epoch 5/1000\n",
      "1146/1146 [==============================] - 263s 230ms/step - loss: 0.8717 - mean_absolute_error: 7.4088 - mean_squared_error: 223.0774 - _pinball_loss: 0.8717 - val_loss: 1.0535 - val_mean_absolute_error: 9.0499 - val_mean_squared_error: 322.1768 - val__pinball_loss: 1.0535\n",
      "Epoch 6/1000\n",
      "1146/1146 [==============================] - 265s 232ms/step - loss: 0.8470 - mean_absolute_error: 7.2473 - mean_squared_error: 220.5212 - _pinball_loss: 0.8470 - val_loss: 1.0259 - val_mean_absolute_error: 8.6647 - val_mean_squared_error: 308.9952 - val__pinball_loss: 1.0259\n",
      "Epoch 7/1000\n",
      "1146/1146 [==============================] - 263s 230ms/step - loss: 0.8235 - mean_absolute_error: 7.0882 - mean_squared_error: 217.6539 - _pinball_loss: 0.8235 - val_loss: 1.0015 - val_mean_absolute_error: 8.5140 - val_mean_squared_error: 306.7207 - val__pinball_loss: 1.0015\n",
      "Epoch 8/1000\n",
      "1146/1146 [==============================] - 311s 272ms/step - loss: 0.8063 - mean_absolute_error: 6.9685 - mean_squared_error: 215.1061 - _pinball_loss: 0.8063 - val_loss: 1.0381 - val_mean_absolute_error: 9.0587 - val_mean_squared_error: 324.5799 - val__pinball_loss: 1.0381\n",
      "Epoch 9/1000\n",
      "1146/1146 [==============================] - 363s 317ms/step - loss: 0.7931 - mean_absolute_error: 6.8855 - mean_squared_error: 213.7374 - _pinball_loss: 0.7931 - val_loss: 0.9830 - val_mean_absolute_error: 8.8463 - val_mean_squared_error: 318.2766 - val__pinball_loss: 0.9830\n",
      "Epoch 10/1000\n",
      "1146/1146 [==============================] - 362s 316ms/step - loss: 0.7818 - mean_absolute_error: 6.8122 - mean_squared_error: 212.2171 - _pinball_loss: 0.7818 - val_loss: 0.9845 - val_mean_absolute_error: 8.1826 - val_mean_squared_error: 298.6965 - val__pinball_loss: 0.9845\n",
      "Epoch 11/1000\n",
      "1146/1146 [==============================] - 361s 315ms/step - loss: 0.7725 - mean_absolute_error: 6.7430 - mean_squared_error: 210.7010 - _pinball_loss: 0.7725 - val_loss: 0.9598 - val_mean_absolute_error: 8.0015 - val_mean_squared_error: 293.8710 - val__pinball_loss: 0.9598\n",
      "Epoch 12/1000\n",
      "1146/1146 [==============================] - 361s 315ms/step - loss: 0.7609 - mean_absolute_error: 6.6654 - mean_squared_error: 209.2450 - _pinball_loss: 0.7609 - val_loss: 0.9653 - val_mean_absolute_error: 8.2140 - val_mean_squared_error: 301.4631 - val__pinball_loss: 0.9653\n",
      "Epoch 13/1000\n",
      "1146/1146 [==============================] - 338s 295ms/step - loss: 0.7546 - mean_absolute_error: 6.6223 - mean_squared_error: 208.3892 - _pinball_loss: 0.7546 - val_loss: 0.9479 - val_mean_absolute_error: 7.9904 - val_mean_squared_error: 293.3451 - val__pinball_loss: 0.9479\n",
      "Epoch 14/1000\n",
      "1146/1146 [==============================] - 265s 231ms/step - loss: 0.7487 - mean_absolute_error: 6.5804 - mean_squared_error: 207.4299 - _pinball_loss: 0.7487 - val_loss: 0.9563 - val_mean_absolute_error: 8.0063 - val_mean_squared_error: 294.8924 - val__pinball_loss: 0.9563\n",
      "Epoch 15/1000\n",
      "1146/1146 [==============================] - 263s 229ms/step - loss: 0.7403 - mean_absolute_error: 6.5222 - mean_squared_error: 206.3093 - _pinball_loss: 0.7403 - val_loss: 0.9391 - val_mean_absolute_error: 8.3291 - val_mean_squared_error: 307.0670 - val__pinball_loss: 0.9391\n",
      "Epoch 16/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 0.7345 - mean_absolute_error: 6.4881 - mean_squared_error: 205.4205 - _pinball_loss: 0.7345 - val_loss: 0.9512 - val_mean_absolute_error: 7.7370 - val_mean_squared_error: 283.6495 - val__pinball_loss: 0.9512\n",
      "Epoch 17/1000\n",
      "1146/1146 [==============================] - 264s 231ms/step - loss: 0.7312 - mean_absolute_error: 6.4632 - mean_squared_error: 204.7532 - _pinball_loss: 0.7312 - val_loss: 0.9453 - val_mean_absolute_error: 7.9495 - val_mean_squared_error: 292.3123 - val__pinball_loss: 0.9453\n",
      "Epoch 18/1000\n",
      "1146/1146 [==============================] - 306s 267ms/step - loss: 0.7237 - mean_absolute_error: 6.4017 - mean_squared_error: 203.5331 - _pinball_loss: 0.7237 - val_loss: 0.9327 - val_mean_absolute_error: 7.8930 - val_mean_squared_error: 289.8111 - val__pinball_loss: 0.9327\n",
      "Epoch 19/1000\n",
      "1146/1146 [==============================] - 277s 241ms/step - loss: 0.7198 - mean_absolute_error: 6.3795 - mean_squared_error: 202.9193 - _pinball_loss: 0.7198 - val_loss: 0.9340 - val_mean_absolute_error: 7.9078 - val_mean_squared_error: 291.0332 - val__pinball_loss: 0.9340\n",
      "Epoch 20/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 0.7155 - mean_absolute_error: 6.3550 - mean_squared_error: 202.4422 - _pinball_loss: 0.7155 - val_loss: 0.9281 - val_mean_absolute_error: 8.0380 - val_mean_squared_error: 302.1860 - val__pinball_loss: 0.9281\n",
      "Epoch 21/1000\n",
      "1146/1146 [==============================] - 263s 230ms/step - loss: 0.7127 - mean_absolute_error: 6.3287 - mean_squared_error: 201.7505 - _pinball_loss: 0.7127 - val_loss: 0.9077 - val_mean_absolute_error: 8.1183 - val_mean_squared_error: 301.8106 - val__pinball_loss: 0.9077\n",
      "Epoch 22/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 0.7092 - mean_absolute_error: 6.3082 - mean_squared_error: 201.1518 - _pinball_loss: 0.7092 - val_loss: 0.9280 - val_mean_absolute_error: 7.7675 - val_mean_squared_error: 287.8058 - val__pinball_loss: 0.9280\n",
      "Epoch 23/1000\n",
      "1146/1146 [==============================] - 339s 296ms/step - loss: 0.7043 - mean_absolute_error: 6.2750 - mean_squared_error: 200.6083 - _pinball_loss: 0.7043 - val_loss: 0.9302 - val_mean_absolute_error: 7.6641 - val_mean_squared_error: 284.8539 - val__pinball_loss: 0.9302\n",
      "Epoch 24/1000\n",
      "1146/1146 [==============================] - 264s 230ms/step - loss: 0.7020 - mean_absolute_error: 6.2585 - mean_squared_error: 200.2016 - _pinball_loss: 0.7020 - val_loss: 0.9110 - val_mean_absolute_error: 8.0195 - val_mean_squared_error: 299.9245 - val__pinball_loss: 0.9110\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000269E5642EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import src.model.multiple_output.convolution\n",
    "import importlib\n",
    "importlib.reload(src.model.multiple_output.convolution)\n",
    "\n",
    "from src.model.multiple_output.convolution import ConvolutionVarious\n",
    "\n",
    "days = 1\n",
    "\n",
    "one_days_window_label_columns = WindowGenerator(\n",
    "    train_df,\n",
    "    valid_df,\n",
    "    test_df,\n",
    "    input_width=48,\n",
    "    label_width=96,\n",
    "    shift=96,\n",
    "    sequence_stride=1,\n",
    "    label_columns=[\"TARGET\"]\n",
    "    \n",
    ")\n",
    "submission_df_1 = load_submission_data()\n",
    "_, predict_df = load_basic_preprocessed_predict(\"minmax\")\n",
    "predict_df[\"scaled_TARGET\"] = predict_df[\"TARGET\"]\n",
    "predict_df = predict_df[cutter]\n",
    "predict_df.drop(\"TARGET\", axis=1, inplace=True)\n",
    "predict_df = load_test_features(predict_df, 48 * days)\n",
    "\n",
    "evaluate_dict_1 = {}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    q = i/10\n",
    "    print(q)\n",
    "    \n",
    "    conv_various = ConvolutionVarious(48)\n",
    "\n",
    "    compile_and_fit_with_pinball_loss(conv_various, one_days_window_label_columns, q)\n",
    "    evaluate_dict_1[q] = conv_various.evaluate(one_days_window_label_columns.test, verbose=0)\n",
    "    predict_np = predict_df.reshape(-1, 48, train_df.shape[-1] - 1)\n",
    "    pred_y = conv_various.predict(predict_np)[:, :, -1]\n",
    "    submission_df_1[f\"q_{q}\"] = pred_y.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: [0.8651361465454102,\n",
       "  6.411491870880127,\n",
       "  160.1835479736328,\n",
       "  0.8650573492050171],\n",
       " 0.2: [1.3709611892700195,\n",
       "  4.5218658447265625,\n",
       "  82.00414276123047,\n",
       "  1.3708319664001465],\n",
       " 0.3: [1.598541498184204,\n",
       "  3.527270793914795,\n",
       "  59.535499572753906,\n",
       "  1.5983389616012573],\n",
       " 0.4: [1.6216026544570923,\n",
       "  3.2097091674804688,\n",
       "  55.50996780395508,\n",
       "  1.6215879917144775],\n",
       " 0.5: [1.569057583808899,\n",
       "  3.138113021850586,\n",
       "  63.68381881713867,\n",
       "  1.5692576169967651],\n",
       " 0.6: [1.3627004623413086,\n",
       "  3.1574835777282715,\n",
       "  74.25511169433594,\n",
       "  1.362777829170227],\n",
       " 0.7: [1.1704150438308716,\n",
       "  3.3607945442199707,\n",
       "  84.16575622558594,\n",
       "  1.1703453063964844],\n",
       " 0.8: [0.8869102597236633,\n",
       "  3.9626379013061523,\n",
       "  100.3033676147461,\n",
       "  0.8868370056152344],\n",
       " 0.9: [0.5113241672515869,\n",
       "  4.614569664001465,\n",
       "  120.91699981689453,\n",
       "  0.5112897157669067]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.217405445045895"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for v in evaluate_dict_1.values():\n",
    "    s += v[0]\n",
    "\n",
    "s/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission_df_1[0:48]\n",
    "submission_df_1.to_csv(\"cnn_many_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7776 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  q_0.1  q_0.2  q_0.3  q_0.4  q_0.5  q_0.6  q_0.7  q_0.8  q_0.9\n",
       "0     0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "1     0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "2     0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "3     0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "4     0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "...  ..    ...    ...    ...    ...    ...    ...    ...    ...    ...\n",
       "7771  0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "7772  0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "7773  0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "7774  0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "7775  0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
       "\n",
       "[7776 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = submission_df_1['id']\n",
    "for i in range(1, 10):\n",
    "    q = i/10\n",
    "    submission_df_1[submission_df_1[f'q_{q}'] < 0] = 0\n",
    "submission_df_1['id'] = _id\n",
    "submission_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1['id'] = submission_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1.to_csv('many_features_1d_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (dacon)",
   "language": "python",
   "name": "pycharm-549c67b6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
